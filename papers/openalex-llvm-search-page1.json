{
  "source": {
    "slug": "openalex-llvm-search-page1",
    "name": "OpenAlex Query (title/abstract: llvm, page 1)",
    "url": "https://openalex.org/works?page=1&filter=title_and_abstract.search:llvm,type:article|dissertation|book-chapter|book&sort=publication_year:desc&per_page=100"
  },
  "papers": [
    {
      "id": "openalex-w7125917504",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "VERIS: A Secure, Readable, and Ownership-Based Programming Language for General-Purpose Software Development",
      "authors": [
        {
          "name": "Ghann 'Patricia",
          "affiliation": "Koforidua Technical University"
        },
        {
          "name": "Owusu Jnr Isaac Amoako",
          "affiliation": "Koforidua Technical University"
        }
      ],
      "year": "2026",
      "publication": "Zenodo (CERN European Organization for Nuclear Research)",
      "venue": "Zenodo (CERN European Organization for Nuclear Research)",
      "type": "research-paper",
      "abstract": "This repository contains the complete research artefacts supporting the paper entitled: “VERIS: A Secure, Efficient, Hybrid General-Purpose Programming Language” The materials I've shared here are meant to ensure full transparency, reproducibility, and reusability of the results reported in the paper. The dataset includes the formal language specification, compiler scaffold, example source programs, benchmark outputs, and system architecture diagrams referenced in the manuscript. Contents of the Repository Language Specification: Detailed documentation describing the design goals, syntax, type system, control flow mechanisms, and security model of the VERIS programming language. Compiler Scaffold: Reference implementation outlining the VERIS compilation pipeline, including front-end parsing and LLVM IR integration. Example Programs: Sample VERIS source files demonstrating core language features such as immutability, control flow, and basic input/output. Benchmark Results: Performance measurements used to support the evaluation section of the paper. Architecture Diagrams: Visual representations of the VERIS system and compiler architecture as referenced in the manuscript. Purpose and Reuse The artefacts in this repository enable independent researchers to: Reproduce the experimental results reported in the paper Inspect and evaluate the VERIS language design Extend or adapt the VERIS compiler framework for further research Licence All materials are released under the MIT License, permitting unrestricted reuse, modification, and distribution with appropriate attribution. Relation to the Article This dataset directly supports the findings and analyses presented in the associated journal article and satisfies the Open Data requirements of the publisher.",
      "paperUrl": "https://doi.org/10.5281/zenodo.18399223",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Performance",
        "Security"
      ]
    },
    {
      "id": "openalex-w7125974770",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Type Deduction Analysis: Reconstructing Transparent Pointer Types in LLVM-IR",
      "authors": [
        {
          "name": "Niccolò Nicolosi",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Gabriele Magnani",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Emilio Corigliano",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Davide Baroffio",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Federico Reghenzani",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Giovanni Agosta",
          "affiliation": "Politecnico di Milano"
        }
      ],
      "year": "2026",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "With version 17, LLVM finalized the transition to opaque pointer types, eliminating explicit pointee‑type information from the Intermediate Representation (IR). Thus, starting from LLVM 17, each pointer type is represented in IR by the unique type ptr. Despite eliminating redundant pointer bitcasts and consequently reducing IR size and compile time, this change disrupts analyses that have reason to rely on pointee-type information, forcing existing compiler projects to depend on outdated LLVM versions. This information can in fact be insightful in fields like approximate computing, where the compiler can apply non-conservative optimizations, or in passes that require it to make analyses and transformations that do not impact the correctness of the program. To address this problem, we present a new Type Deduction Analysis pass that reconstructs transparent pointer types directly from opaque‑pointer IR. Moreover, we illustrate two different case-studies on existing LLVM projects, namely TAFFO and ASPIS, that demonstrate the need for pointee-type information in LLVM compilers.",
      "paperUrl": "https://doi.org/10.1145/3771775.3786268",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Optimizations"
      ]
    },
    {
      "id": "openalex-w7118082668",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Temporal Property Verification in Plugin-based Software by Program Analysis",
      "authors": [
        {
          "name": "Xavier Noumbissi Noundou",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "publication": "Zenodo (CERN European Organization for Nuclear Research)",
      "venue": "Zenodo (CERN European Organization for Nuclear Research)",
      "type": "research-paper",
      "abstract": "Temporal Property Verification in Plugin-based Software by Program Analysis 1. Information Brochure of the Design and Testing System YEROTH_QVGE (YR_QVGE): https://www.zenodo.org/record/8045734/ 2. YR_DB_RUNTIME_VERIF: A FRAMEWORK FOR VERIFYING SQL CORRECTNESS PROPERTIES OF GUI SOFTWARE AT RUNTIME: https://www.zenodo.org/record/8051303/ 3. SAINT: Simple Static Taint Analysis Tool User's manual: https://www.zenodo.org/record/8051299/ 4. Context-Sensitive Staged Static Taint Analysis for C using LLVM: https://www.zenodo.org/record/8051293/",
      "paperUrl": "https://doi.org/10.5281/zenodo.18133151",
      "sourceUrl": "",
      "tags": [
        "Testing"
      ]
    },
    {
      "id": "openalex-w7128661364",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Symbolic debugging of optimized code: measuring, testing, tuning and enhancing debug information quality",
      "authors": [
        {
          "name": "Cristian Assaiante",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "publication": "IRIS Research product catalog (Sapienza University of Rome)",
      "venue": "IRIS Research product catalog (Sapienza University of Rome)",
      "type": "thesis",
      "abstract": "Compiler optimizations often disrupt the correspondence between source code and generated binaries, undermining the effectiveness of symbolic debugging, which relies on debug information to encode this mapping. Preserving source-level constructs under aggressive transformations remains a formidable challenge: optimizations that restructure control flow, inline functions, or instruction scheduling frequently lead to missing or misleading metadata, causing debuggers to display incomplete or inaccurate program states. While recent works have exposed numerous bugs in compiler toolchains and proposed validation methodologies to detect inconsistencies in debug symbols, the research community still lacks a quantitative and systematic understanding of how much source-level information is preserved after optimization and how much of it could be recovered or retained through more comprehensive compiler testing and analysis. This thesis addresses this gap through a set of complementary contributions that make the debugging quality of optimized code both measurable and improvable. First, it introduces a hybrid methodology to quantify debug information quality, combining dynamic traces and static analysis to accurately measure the completeness of preserved lines and variables. Second, it proposes a conjecture-based approach to detect completeness bugs in compiler toolchains, cases where compilers silently omit variables from the debug state, and reports 38 previously unknown issues in gcc and clang, 24 of which were confirmed and fixed by developers. Third, it presents DebugTuner, a framework that tunes compiler optimizations to preserve debuggability by identifying individual passes that degrade debug information and synthesizing debug-friendly configurations with minimal performance loss. This work also led to the introduction of the -opt-disable flag in LLVM, now available to the community. Finally, the thesis introduces Reparo, a binary-level analysis tool that enhances existing debug information, reconstructing accurate variable lifetimes that reflect the computations in binary code. The same analysis methodology is further extended to API monitoring for malware analysis, improving completeness and correctness in such systems, proving the cross-domain capability of our research. Together, these contributions transform symbolic debugging from a best-effort, tool-specific capability into a measurable, improvable property of modern compilation. They provide a foundation for developing compilers and analysis tools that optimize not only for performance but also for debuggability.",
      "paperUrl": "https://hdl.handle.net/11573/1759839",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Debug Information",
        "Optimizations",
        "Performance",
        "Static Analysis",
        "Testing"
      ]
    },
    {
      "id": "openalex-w7125352923",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Static Detection of Core Structures in Tigress Virtualization-Based Obfuscation Using an LLVM Pass",
      "authors": [
        {
          "name": "Sangjun An",
          "affiliation": ""
        },
        {
          "name": "Seoksu Lee",
          "affiliation": ""
        },
        {
          "name": "Eun-Sun Cho",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "publication": "ArXiv.org",
      "venue": "ArXiv.org",
      "type": "research-paper",
      "abstract": "Malware often uses obfuscation to hinder security analysis. Among these techniques, virtualization-based obfuscation is particularly strong because it protects programs by translating original instructions into attacker-defined virtual machine (VM) bytecode, producing long and complex code that is difficult to analyze and deobfuscate. This paper aims to identify the structural components of virtualization-based obfuscation through static analysis. By examining the execution model of obfuscated code, we define and detect the key elements required for deobfuscation-namely the dispatch routine, handler blocks, and the VM region-using LLVM IR. Experimental results show that, in the absence of compiler optimizations, the proposed LLVM Pass successfully detects all core structures across major virtualization options, including switch, direct, and indirect modes.",
      "paperUrl": "https://arxiv.org/pdf/2601.12916",
      "sourceUrl": "http://arxiv.org/abs/2601.12916",
      "tags": [
        "IR",
        "Optimizations",
        "Security",
        "Static Analysis"
      ]
    },
    {
      "id": "openalex-w7129530006",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Real Function Approximation Around a Point with Taylor Polynomials Using Custom Machine Code",
      "authors": [
        {
          "name": "Andreja Janković",
          "affiliation": "University of Belgrade"
        },
        {
          "name": "Dragan Bojić",
          "affiliation": "University of Belgrade"
        }
      ],
      "year": "2026",
      "publication": "Lecture notes in networks and systems",
      "venue": "Lecture notes in networks and systems",
      "type": "research-paper",
      "abstract": "No abstract available in OpenAlex metadata.",
      "paperUrl": "https://doi.org/10.1007/978-3-032-04890-5_34",
      "sourceUrl": "",
      "tags": []
    },
    {
      "id": "openalex-w7125913748",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "RIFS: Run-Time Invariant Function Specialization",
      "authors": [
        {
          "name": "Saba Jamilan",
          "affiliation": "University of California, Santa Cruz"
        },
        {
          "name": "Snehasish Kumar",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Heiner Litz",
          "affiliation": "University of California, Santa Cruz"
        }
      ],
      "year": "2026",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "Compilers apply optimizations such as function specialization and constant propagation to eliminate redundant work at compile time. However, because compilers must prove that values are constant, many profitable optimization opportunities remain unrealized. In this paper, we propose run-time invariant function specialization (RIFS), a profile-guided compiler technique that specializes functions based on runtime invariant call-site-specific argument values. RIFS introduces a novel value-profiling LLVM pass to identify runtime invariant arguments, even though they cannot be proven constant statically. A subsequent LLVM transformation pass generates specialized function variants tailored to these value profiles. To efficiently select among potentially thousands of specialization candidates, we develop a predictive cost model that estimates the performance benefit of each candidate prior to code generation. We integrate our passes seamlessly into the existing PGO-enabled LLVM toolchain. We evaluate RIFS across 11 real-world applications, demonstrating substantial improvements over state-of-the-art optimization techniques. RIFS achieves an average speedup of 6.3% and an instruction reduction of 2.5% over the LLVM -O3+PGO baseline.",
      "paperUrl": "https://doi.org/10.1145/3771775.3786274",
      "sourceUrl": "",
      "tags": [
        "Optimizations",
        "Performance",
        "PGO"
      ]
    },
    {
      "id": "openalex-w7111544135",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Quantifying Compiler-induced Reliability Loss in Software-Implemented Hardware Fault Tolerance",
      "authors": [
        {
          "name": "Davide Baroffio",
          "affiliation": ""
        },
        {
          "name": "Johannes Geier",
          "affiliation": ""
        },
        {
          "name": "Federico Reghenzani",
          "affiliation": ""
        },
        {
          "name": "Ulf Schlichtmann",
          "affiliation": ""
        },
        {
          "name": "William Fornaciari",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "publication": "Virtual Community of Pathological Anatomy (University of Castilla La Mancha)",
      "venue": "Virtual Community of Pathological Anatomy (University of Castilla La Mancha)",
      "type": "research-paper",
      "abstract": "Compiler mechanisms for Software-Implemented Hardware Fault Tolerance (SIHFT) offer a cost-effective solution for reliability, paving the way towards the adoption of Commercial Off-The-Shelf (COTS) components in safety-critical environments. However, default compiler optimizations can remove the SIHFT-induced redundancy and checks. For this reason, the use of compiler optimizations was discouraged in the literature. This article presents a comprehensive study of the reliability degradation introduced by LLVM's O2 optimization pipeline when using a state-of-the-art SIHFT tool. We quantify, via RTL fault injection, the impact of O2 at different optimization stages, which identified a data corruption rate increase by up to 48x. We also propose a static exploration methodology to identify the LLVM passes that harm the reliability. Then, we remove these harmful passes from the optimization pipeline, demonstrating how to tune optimization pipelines to make SIHFT successful even in presence of compiler optimizations.",
      "paperUrl": "https://hdl.handle.net/11311/1298227",
      "sourceUrl": "",
      "tags": [
        "Optimizations"
      ]
    },
    {
      "id": "openalex-w7128991505",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine Learning",
      "authors": [
        {
          "name": "Risheng Xu",
          "affiliation": "Mercedes-Benz (Germany)"
        },
        {
          "name": "Philipp Sieweck",
          "affiliation": "Christian-Albrechts-Universität zu Kiel"
        },
        {
          "name": "Hermann von Hasseln",
          "affiliation": "Mercedes-Benz (Germany)"
        },
        {
          "name": "Dirk Nowotka",
          "affiliation": "Christian-Albrechts-Universität zu Kiel"
        }
      ],
      "year": "2026",
      "publication": "SPIRE - Sciences Po Institutional REpository",
      "venue": "SPIRE - Sciences Po Institutional REpository",
      "type": "research-paper",
      "abstract": "International audience",
      "paperUrl": "https://doi.org/10.82331/erts.2026.30",
      "sourceUrl": "",
      "tags": []
    },
    {
      "id": "openalex-w7119532938",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Performance analysis of Arm-based processors across multiple compilers for HPC workloads.",
      "authors": [
        {
          "name": "Chigusa Kobayashi",
          "affiliation": "RIKEN Center for Computational Science"
        },
        {
          "name": "Kazuto Ando",
          "affiliation": "RIKEN Center for Computational Science"
        },
        {
          "name": "Tsuyoshi Yamaura",
          "affiliation": "RIKEN Center for Computational Science"
        },
        {
          "name": "Hikaru Inoue",
          "affiliation": "RIKEN Center for Computational Science"
        },
        {
          "name": "Hitoshi Murai",
          "affiliation": "RIKEN Center for Computational Science"
        }
      ],
      "year": "2026",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "Arm-based processors are becoming prominent in the high-performance computing (HPC) field. Notable examples include Fujitsu’s A64FX, Amazon Web Services’ Graviton series, and Nvidia’s Grace processor. While the Arm Scalable Vector Extension (SVE) has been implemented in open-source-software compilers such as GNU and LLVM, the practical performance implications for real HPC applications remain insufficiently explored. We conducted performance analyses of various HPC workloads using multiple compilers, including both open-source and proprietary ones, across four Arm-based CPUs: A64FX, Graviton3E, Graviton4, and Grace. To enable consistent profiling across heterogeneous Arm CPUs, we also developed a wrapper for the Linux perf profiling utility that provides a unified interface. This tool enables us to uniformly analyze the key metrics such as SVE use and cache behavior, and it facilitated systematic comparisons of performance across architectures and compilers. Our analyses revealed several performance-related limitations in the current state of the tested compilers, while also providing guidance for the tuning of real applications. These findings provide practical insights for optimizing real HPC workloads with the HPC potential of Arm-based processors and also highlight critical factors that must be addressed to achieve optimal performance when porting applications from other architectures.",
      "paperUrl": "https://doi.org/10.1145/3773656.3773671",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ]
    },
    {
      "id": "openalex-w7127177283",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "MoreFit",
      "authors": [
        {
          "name": "Christoph Langenbruch",
          "affiliation": "Heidelberg University"
        }
      ],
      "year": "2026",
      "publication": "The European Physical Journal C",
      "venue": "The European Physical Journal C | Vol. 86 (Issue 2)",
      "type": "research-paper",
      "abstract": "Abstract Parameter estimation via unbinned maximum likelihood fits is a central technique in particle physics. This article introduces MoreFit, which aims to provide a more optimised, rapid and efficient fitting solution for unbinned maximum likelihood fits. MoreFit is developed with a focus on parallelism and relies on computation graphs that are compiled just-in-time. Several novel automatic optimisation techniques are employed on the computation graphs that significantly increase performance compared to conventional approaches. MoreFit can make efficient use of a wide range of heterogeneous platforms through its compute backends that rely on open standards. It provides an OpenCL backend for execution on GPUs of all major vendors, and a backend based on LLVM and Clang for single- or multithreaded execution on CPUs, which in addition allows for SIMD vectorisation. MoreFit is benchmarked against several other fitting frameworks and shows very promising performance, illustrating the power of the approach.",
      "paperUrl": "https://doi.org/10.1140/epjc/s10052-026-15326-7",
      "sourceUrl": "",
      "tags": [
        "Backend",
        "Clang",
        "OpenCL",
        "Performance"
      ]
    },
    {
      "id": "openalex-w7126053170",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Microarchitectural comparison, in-core modeling, and memory hierarchy analysis of state-of-the-art CPUs: Grace, Sapphire Rapids, and Genoa",
      "authors": [
        {
          "name": "Jan Laukemann",
          "affiliation": "Friedrich-Alexander-Universität Erlangen-Nürnberg"
        },
        {
          "name": "Georg Hager",
          "affiliation": "Friedrich-Alexander-Universität Erlangen-Nürnberg"
        },
        {
          "name": "Gerhard Wellein",
          "affiliation": "Friedrich-Alexander-Universität Erlangen-Nürnberg"
        }
      ],
      "year": "2026",
      "publication": "Parallel Computing",
      "venue": "Parallel Computing | Vol. 127",
      "type": "research-paper",
      "abstract": "Three big semiconductor companies in HPC are currently competing in the race for the best CPU: AMD, Intel, and NVIDIA. There are significant differences among their state-of-the-art CPU designs, spanning the entire range from instruction execution to cache behavior and main memory bandwidth. In this work, we analyze the performance of CPUs based on the Zen 4, Golden Cove, and Neoverse V2 microarchitectures. We create accurate in-core performance models for use with the Open Source Architecture Code Analyzer (OSACA) tool and compare its prediction accuracy with llvm-mca. Beyond the tool aspect, this reveals interesting differences in in-core design points but also some commonalities. Beyond the single core, we extend our comparison by measuring data-transfer behavior through the memory hierarchy using a variety of microbenchmarks. We thoroughly investigate the “write-allocate (WA) evasion” feature, which can automatically reduce the memory traffic caused by write misses. We show that the Grace Superchip has a next-to-optimal implementation of WA evasion while the Sapphire Rapids CPU can avoid write allocates completely only in specific scenarios. The only way to eliminate WAs on AMD Genoa is the explicit use of non-temporal stores. Finally, we study the cache hierarchy of the CPUs in view of the Execution-Cache-Memory (ECM) performance model, revealing overlapping cache hierarchies on Genoa and Grace in contrast to Sapphire Rapids.",
      "paperUrl": "https://doi.org/10.1016/j.parco.2026.103183",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ]
    },
    {
      "id": "openalex-w7126388686",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "MEĐUREPREZENTACIJE IZVORNOG KODA RUSTC KOMPAJLERA",
      "authors": [
        {
          "name": "Aleksa Bajat",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "publication": "Zbornik radova Fakulteta tehničkih nauka u Novom Sadu",
      "venue": "Zbornik radova Fakulteta tehničkih nauka u Novom Sadu | Vol. 40 (Issue 12)",
      "type": "research-paper",
      "abstract": "This paper explores the architecture of the Rust compiler's (rustc) frontend, focusing on the crucial role of intermediate representations (IRs) of source code in the compilation process. Phases from lexical analysis and parsing to the generation of the Abstract Syntax Tree (AST), through the High-Level Intermediate Representation (HIR), Typed HIR (THIR), to the Mid-Level Intermediate Representation (MIR) are analyzed. The paper explains how each of these representations enables key features of the Rust language, including type checking, error diagnostics, incremental compilation, borrow checking, and preparation for further optimization and code generation in LLVM. The aim is to demonstrate how generation of compiler infrastructure contributes to Rust's guarantees of memory safety and high performance.",
      "paperUrl": "https://zbornik.ftn.uns.ac.rs/index.php/zbornik/article/download/4144/3719",
      "sourceUrl": "https://doi.org/10.24867/33be08bajat",
      "tags": [
        "Frontend",
        "Infrastructure",
        "Performance",
        "Rust"
      ]
    },
    {
      "id": "openalex-w7128543793",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "LLM-IARE: An Input-Aware Resilience Estimation Methodology for LLMs under Hardware Transient Faults",
      "authors": [
        {
          "name": "Jiajia Jiao",
          "affiliation": "Shanghai Maritime University"
        },
        {
          "name": "Tainian Zhou",
          "affiliation": "Shanghai Maritime University"
        },
        {
          "name": "Ran Wen",
          "affiliation": "Shanghai Maritime University"
        },
        {
          "name": "Yulian Li",
          "affiliation": "Shanghai Maritime University"
        },
        {
          "name": "Jin Liu",
          "affiliation": "Shanghai University"
        }
      ],
      "year": "2026",
      "publication": "ACM Transactions on Design Automation of Electronic Systems",
      "venue": "ACM Transactions on Design Automation of Electronic Systems",
      "type": "research-paper",
      "abstract": "Large Language Models (LLMs) are being increasingly applied in various natural language processing tasks including safety-critical systems (e.g., medical diagnosis querying, and code generation for self-driving), where resilience to hardware transient faults is essential for guaranteed safety. Traditional Fault Injection (FI) approaches are time consuming due to a large number of repeated executions, which limits their scalability to fast evaluate large-scale resilience. To address these challenges, we propose LLM-IARE, a novel Input-Aware Resilience Estimation Model for LLMs under hardware transient faults. It takes advantage of twice static analysis and once dynamic execution to extract critical parameters to compute general resilience metrics such as Silent Data Corruption (SDC) rates. Fast static analysis can obtain the primary LLM parameters, while dynamic execution can provide input-sensitive attention profiling to characterize how input variations influence internal attention patterns dynamically. More importantly, our proposed LLM-IARE uses the obtained parameters for modeling at three levels (operation, module, and layer) so that the SDC rates of transient fault impacts on LLMs can be calculated quickly and accurately. Additionally, LLM-IARE is further extended to estimate the LLM application-level resilience metric, the cosine similarity reflecting the bit-upset induced semantic fault impacts on final output quality. Comprehensive experiments on six representative LLMs (for example, GPT-2, T5 and RoBERTa), and 30 BERT variants demonstrate that LLM-IARE achieves a fast and accurate LLMs resilience evaluation, with up to 7335 × (average 4500 ×) speedup and an average logarithmic relative error of 3.94% compared to advanced LLVM-based fault injection methods. We further extend the evaluation to four larger Qwen2.5 models (0.5B–7B), where LLM-IARE maintains stable accuracy with logarithmic relative errors between 1.96% and 4.21% (average 3.39%).",
      "paperUrl": "https://doi.org/10.1145/3796531",
      "sourceUrl": "",
      "tags": [
        "Static Analysis"
      ]
    },
    {
      "id": "openalex-w7126002907",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Kontrollflödesintegritet i praktiken : retrospektiv, verklighet och automatiserad tillämpning",
      "authors": [
        {
          "name": "Sabine Houy",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "publication": "DiVA at Umeå University (Umeå University)",
      "venue": "DiVA at Umeå University (Umeå University)",
      "type": "research-paper",
      "abstract": "Control Flow Integrity (CFI) is a well-established mitigation against control-flow hijacking attacks arising from memory corruption vulnerabilities. Over the past two decades, numerous CFI mechanisms have been proposed and integrated into modern compilers and software ecosystems. Despite this progress, CFI remains difficult to adopt in practice, and deployment decisions, compatibility constraints, and engineering overhead strongly influence its real-world security impact. This dissertation investigates Control Flow Integrity from the perspective of practical adoption and deployability. Rather than treating CFI as a purely theoretical protection, it examines how CFI is selected, integrated, and maintained in real-world software systems, and why these steps often fall short of idealized designs. The dissertation is structured around four complementary studies that together trace the path from measurement to guidance, to deployment experience, and finally to automated enforcement. The first study presents a large-scale empirical analysis of deployed binaries to assess the current state of LLVM-CFI adoption across major software platforms. It shows that while CFI deployment is increasing in some ecosystems, it remains uneven and limited, leaving substantial portions of the attack surface unprotected. The second study addresses the lack of practical guidance for developers by introducing a systematic taxonomy that maps LLVM-CFI variants to common classes of memory corruption vulnerabilities. This taxonomy provides actionable recommendations to support incremental, informed adoption of CFI in existing codebases. The third study examines the practical challenges of deploying CFI in a complex, production-grade runtime. Through a detailed case study of integrating LLVM-CFI into a modern Java Virtual Machine, it demonstrates that compatibility issues, manual exclusions, and maintenance effort are central obstacles to effective enforcement, even when strong CFI mechanisms are available. These findings highlight the gap between CFI as designed and CFI as deployed. Building on these insights, the dissertation introduces an automated framework for CFI policy generation and enforcement. By reducing manual effort and mitigating compatibility barriers, this approach enables more consistent and scalable CFI deployment across large and evolving software systems. Overall, the dissertation shows that the effectiveness of Control Flow Integrity in practice is shaped less by the availability of CFI mechanisms than by the feasibility of adopting them. By combining empirical measurement, practical guidance, deployment experience, and automation, this work contributes toward a more realistic and actionable understanding of CFI and provides concrete support for improving its deployment in real-world software systems.",
      "paperUrl": "http://urn.kb.se/resolve?urn=urn:nbn:se:umu:diva-248700",
      "sourceUrl": "",
      "tags": [
        "Security"
      ]
    },
    {
      "id": "openalex-w7127883925",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Interaction-aware multi-objective optimization method for LLVM compiler option sequences",
      "authors": [
        {
          "name": "Yuanjie Lai",
          "affiliation": "Fujian Normal University"
        },
        {
          "name": "Shuke Qiao",
          "affiliation": "Handan College"
        },
        {
          "name": "Youcong Ni",
          "affiliation": "Fujian Normal University"
        },
        {
          "name": "Xin Du",
          "affiliation": "Fujian Normal University"
        },
        {
          "name": "RuLiang Xiao",
          "affiliation": "Fujian Normal University"
        },
        {
          "name": "Dingbang Fang",
          "affiliation": "Fujian Normal University"
        }
      ],
      "year": "2026",
      "publication": "Performance Evaluation",
      "venue": "Performance Evaluation | Vol. 171",
      "type": "research-paper",
      "abstract": "No abstract available in OpenAlex metadata.",
      "paperUrl": "https://doi.org/10.1016/j.peva.2026.102543",
      "sourceUrl": "",
      "tags": []
    },
    {
      "id": "openalex-w7118312905",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Integrating Quantum Software Tools with(in) MLIR",
      "authors": [
        {
          "name": "Patrick Hopf",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Erick Ochoa Lopez",
          "affiliation": "Advanced Micro Devices (Canada)"
        },
        {
          "name": "Yannick Stade",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Damian Rovara",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Nils Quetschlich",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Ioan Albert Florea",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Josh Izaac",
          "affiliation": "Xanadu Quantum Technologies (Canada)"
        },
        {
          "name": "Robert Wille",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Lukas Burgholzer",
          "affiliation": "Technical University of Munich"
        }
      ],
      "year": "2026",
      "publication": "arXiv (Cornell University)",
      "venue": "arXiv (Cornell University)",
      "type": "research-paper",
      "abstract": "Compilers transform code into action. They convert high-level programs into executable hardware instructions - a crucial step in enabling reliable and scalable quantum computation. However, quantum compilation is still in its infancy, and many existing solutions are ad hoc, often developed independently and from scratch. The resulting lack of interoperability leads to significant missed potential, as quantum software tools remain isolated and cannot be seamlessly integrated into cohesive toolchains. The Multi-Level Intermediate Representation (MLIR) has addressed analogous challenges in the classical domain. It was developed within the LLVM project, which has long powered robust software stacks and enabled compilation across diverse software and hardware components, with particular importance in high-performance computing environments. However, MLIR's steep learning curve poses a significant barrier to entry, particularly in quantum computing, where much of the software stack is still predominantly built by experimentalists out of necessity rather than by experienced software engineers. This paper provides a practical and hands-on guide for quantum software engineers to overcome this steep learning curve. Through a concrete case study linking Xanadu's PennyLane framework with the Munich Quantum Toolkit (MQT), we outline actionable integration steps, highlight best practices, and share hard-earned insights from real-world development. This work aims to support quantum tool developers in navigating MLIR's complexities and to foster its adoption as a unifying bridge across a rapidly growing ecosystem of quantum software tools, ultimately guiding the development of more modular, interoperable, and integrated quantum software stacks.",
      "paperUrl": "https://doi.org/10.1145/3773656.3773658",
      "sourceUrl": "",
      "tags": [
        "MLIR",
        "Performance",
        "Quantum Computing"
      ]
    },
    {
      "id": "openalex-w7128302604",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "HPC++: An LLVM-Based Automatic Parallelization Framework with Heterogeneous CPU-GPU Execution",
      "authors": [
        {
          "name": "Vahram Martirosyan",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "No abstract available in OpenAlex metadata.",
      "paperUrl": "https://doi.org/10.13140/rg.2.2.21490.77763",
      "sourceUrl": "",
      "tags": [
        "GPU"
      ]
    },
    {
      "id": "openalex-w7127453644",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "General Store: Speculative Address Translation in x86 Processors",
      "authors": [
        {
          "name": "Yanik Kleibrink",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Anirban Chakraborty",
          "affiliation": "Max Planck Institute for Security and Privacy"
        },
        {
          "name": "Yuval Yarom",
          "affiliation": "Ruhr University Bochum"
        }
      ],
      "year": "2026",
      "publication": "Proceedings of the Microarchitecture Security Conference",
      "venue": "Proceedings of the Microarchitecture Security Conference",
      "type": "research-paper",
      "abstract": "The Spectre family of attacks exploits speculative execution to access secret data and transmit it across isolation boundaries using a microarchitectural covert channel. Whereas prior work has predominantly examined the use of speculative loads for constructing such channels, we investigate speculative stores and flush operations across a wide range of Intel and AMD processors. We find that speculative memory operations either update the data cache or initiate page table walks. Depending on the microarchitecture, the walk may complete and update the TLB or be aborted after populating data caches, leaving clear microarchitectural traces of the translation. We further characterize the effects of page table attributes, memory fences, and cache-coherence states on this behavior. Building on these findings, we introduce a covert channel that leverages only the page table walk activity of speculative stores to encode information, without relying on store-induced cache fills. Finally, we demonstrate that Speculative Load Hardening (SLH)---a widely deployed Spectre-v1 mitigation in LLVM---does not prevent speculative store-based leakage of register values, consistent with its threat model and design assumptions.",
      "paperUrl": "https://moving-the-social.ub.rub.de/index.php/uASC/article/download/12706/12396",
      "sourceUrl": "https://doi.org/10.46586/uasc.2026.002",
      "tags": []
    },
    {
      "id": "openalex-w7128049088",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "From Stochastic to Semantic: Advanced Attribute-Guided Compiler Testing",
      "authors": [
        {
          "name": "Jiangchang Wu",
          "affiliation": "Nanjing University"
        },
        {
          "name": "Yibiao Yang",
          "affiliation": "Nanjing University"
        },
        {
          "name": "Maolin Sun",
          "affiliation": "Nanjing University"
        },
        {
          "name": "Qingyang Li",
          "affiliation": "Nanjing University"
        },
        {
          "name": "Kang Chen",
          "affiliation": "Nanjing University"
        },
        {
          "name": "Lei Xu",
          "affiliation": "Nanjing University"
        },
        {
          "name": "Yuming Zhou",
          "affiliation": "Nanjing University"
        }
      ],
      "year": "2026",
      "publication": "ACM Transactions on Software Engineering and Methodology",
      "venue": "ACM Transactions on Software Engineering and Methodology",
      "type": "research-paper",
      "abstract": "Compiler testing is critically important, as compilers serve as the foundational infrastructure in system software development. A comprehensive exploration of the compilation space is essential for uncovering bugs in compilers. Existing methods primarily involve the utilization of various compilation options alongside test programs as inputs for stress-testing compilers. However, these compilation options are typically applied uniformly across all program elements-such as functions and variables, by default, limiting the ability to thoroughly explore the compilation space. In programming languages like C and C++, attributes such as the __attribute__((always_inline)) directive provide a mechanism for programmers to specify additional information for specific code elements to the compiler. These attributes allow for precise control over the compilation process, such as enforcing constraints and customizing optimization passes for particular elements. This flexibility in specifying attributes offers opportunities to investigate previously unexamined areas within compilers. Unfortunately, few studies have leveraged attributes for compiler testing. To this end, we propose Atlas, an attribute-guided approach that strategically inserts attributes into test programs to facilitate a more thorough exploration of the compilation space. Our key insight is that attributes specified for individual program elements can provide a more flexible means of exploring the compilation space. Our extensive experiments on GCC and LLVM demonstrate the superiority of Atlas over baseline testing techniques that do not employ attributes, particularly in terms of bug detection and code coverage. Furthermore, Atlas has led to the discovery of 97 unique bugs in GCC and LLVM, 73 of which have already been confirmed or fixed, showcasing its practical utility",
      "paperUrl": "https://doi.org/10.1145/3793554",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Infrastructure",
        "Programming Languages",
        "Testing"
      ]
    },
    {
      "id": "openalex-w7119468303",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Enhancing Symbolic Execution with Machine-Checked Safety Proofs",
      "authors": [
        {
          "name": "David Trabish",
          "affiliation": "Technion – Israel Institute of Technology"
        },
        {
          "name": "Shachar Itzhaky",
          "affiliation": "Technion – Israel Institute of Technology"
        }
      ],
      "year": "2026",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "Symbolic execution (SE) is a program analysis technique that executes the program with symbolic inputs. In modern SE engines, when the analysis of a given program is exhaustive, the analyzed program is typically considered safe, i.e., free of bugs, but no formal guarantees are provided to support this. Rather than aiming for a formally verified SE engine that will provide such guarantees, which is challenging, we propose a systematic approach where each individual analysis additionally generates a formal safety proof that validates the symbolic computations that were carried out. Our approach consists of two main components: A formal framework connecting concrete and symbolic semantics, and an instrumentation of the SE engine which generates formal safety proofs based on this framework. We showcase our approach by implementing a KLEE-based prototype that operates on a subset of LLVM IR with integers and generates proofs in Rocq. Our preliminary experiments show that our approach generates proofs that have reasonable validation times, while the instrumentation incurs only a minor overhead on the SE engine. In addition, during the implementation of our prototype, we found previously unknown semantic implementation issues in KLEE.",
      "paperUrl": "https://doi.org/10.1145/3779031.3779089",
      "sourceUrl": "",
      "tags": [
        "IR"
      ]
    },
    {
      "id": "openalex-w7127541865",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization",
      "authors": [
        {
          "name": "Haolin Pan",
          "affiliation": ""
        },
        {
          "name": "Lianghong Huang",
          "affiliation": ""
        },
        {
          "name": "Jinyuan Dong",
          "affiliation": ""
        },
        {
          "name": "Mingjie Xing",
          "affiliation": ""
        },
        {
          "name": "Yanjun Wu",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "publication": "ArXiv.org",
      "venue": "ArXiv.org",
      "type": "research-paper",
      "abstract": "Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges interpretable reasoning with combinatorial search. We first propose a reverse engineering methodology to construct a Chain-of-Thought dataset, explicitly mapping static code features to verifiable performance evidence. This enables the model to learn the causal logic governing optimization decisions rather than merely imitating sequences. Leveraging this interpretable prior, we design a collaborative inference mechanism where the LLM functions as a strategist, defining optimization intents that dynamically guide the mutation operations of a genetic algorithm. Experimental results on seven datasets demonstrate that ECCO significantly outperforms the LLVM opt -O3 baseline, achieving an average 24.44% reduction in cycles.",
      "paperUrl": "https://arxiv.org/pdf/2602.00087",
      "sourceUrl": "http://arxiv.org/abs/2602.00087",
      "tags": [
        "Performance"
      ]
    },
    {
      "id": "openalex-w7125351555",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Discovering 100+ Compiler Defects in 72 Hours via LLM-Driven Semantic Logic Recomposition",
      "authors": [
        {
          "name": "Xinabang He",
          "affiliation": ""
        },
        {
          "name": "Yuanwei Chen",
          "affiliation": ""
        },
        {
          "name": "Hao Wu",
          "affiliation": ""
        },
        {
          "name": "Jikang Zhang",
          "affiliation": ""
        },
        {
          "name": "Zicheng Wang",
          "affiliation": ""
        },
        {
          "name": "Ligeng Chen",
          "affiliation": ""
        },
        {
          "name": "Junjie Peng",
          "affiliation": ""
        },
        {
          "name": "Haiyang Wei",
          "affiliation": ""
        },
        {
          "name": "Qian Yi",
          "affiliation": ""
        },
        {
          "name": "Tiantai Zhang",
          "affiliation": ""
        },
        {
          "name": "Linzhang Wang",
          "affiliation": ""
        },
        {
          "name": "Bing Mao",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "publication": "ArXiv.org",
      "venue": "ArXiv.org",
      "type": "research-paper",
      "abstract": "Compilers constitute the foundational root-of-trust in software supply chains; however, their immense complexity inevitably conceals critical defects. Recent research has attempted to leverage historical bugs to design new mutation operators or fine-tune models to increase program diversity for compiler fuzzing.We observe, however, that bugs manifest primarily based on the semantics of input programs rather than their syntax. Unfortunately, current approaches, whether relying on syntactic mutation or general Large Language Model (LLM) fine-tuning, struggle to preserve the specific semantics found in the logic of bug-triggering programs. Consequently, these critical semantic triggers are often lost, resulting in a limitation of the diversity of generated programs. To explicitly reuse such semantics, we propose FeatureFuzz, a compiler fuzzer that combines features to generate programs. We define a feature as a decoupled primitive that encapsulates a natural language description of a bug-prone invariant, such as an out-of-bounds array access, alongside a concrete code witness of its realization. FeatureFuzz operates via a three-stage workflow: it first extracts features from historical bug reports, synthesizes coherent groups of features, and finally instantiates these groups into valid programs for compiler fuzzing. We evaluated FeatureFuzz on GCC and LLVM. Over 24-hour campaigns, FeatureFuzz uncovered 167 unique crashes, which is 2.78x more than the second-best fuzzer. Furthermore, through a 72-hour fuzzing campaign, FeatureFuzz identified 113 bugs in GCC and LLVM, 97 of which have already been confirmed by compiler developers, validating the approach's ability to stress-test modern compilers effectively.",
      "paperUrl": "https://arxiv.org/pdf/2601.12360",
      "sourceUrl": "http://arxiv.org/abs/2601.12360",
      "tags": [
        "Rust"
      ]
    },
    {
      "id": "openalex-w7126089364",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Detecting Hardware Trojans in High-Level Synthesis-Generated RTL using Large Language Models",
      "authors": [
        {
          "name": "Rijoy Mukherjee",
          "affiliation": "Indian Institute of Technology Kharagpur"
        },
        {
          "name": "Rajat Subhra Chakraborty",
          "affiliation": "Indian Institute of Technology Kharagpur"
        }
      ],
      "year": "2026",
      "publication": "ACM Transactions on Design Automation of Electronic Systems",
      "venue": "ACM Transactions on Design Automation of Electronic Systems",
      "type": "research-paper",
      "abstract": "High-Level Synthesis (HLS)-based VLSI design flows allow designers to start from high-level specifications in a general-purpose programming language like C/C++ and automatically generate an optimized hardware design in a hardware description language (HDL) like Verilog or VHDL. Supply of compromised computer-aided design (CAD) tools by an electronic design automation (EDA) vendor to the chip designers is a great threat that adversely affects the horizontal semiconductor business model. Recent works have examined the potential for security issues induced by a compromised HLS CAD tool and demonstrated how HLS is a prime candidate for hardware Trojan (HT) insertion into any underlying design since it is hard to correlate the high-level description to the generated register-transfer level (RTL) code. Further, it has been shown that the use of compiler-generated intermediate representation (IR) as a likely attack vector for inserting HT in the RTL during an HLS-based IC design flow, taking advantage of the lack of automated methods to analyse logic inside a complex LLVM IR. In this work, we propose, implement, and evaluate a novel HLS security verification framework by leveraging the modern large language models (LLMs). Specifically, we focus on detecting the HTs introduced using Black-Hat HLS and HLS-IRT toolchain by performing functional verification using LLM. The experimental results show that LLMs have an impressive ability to analyse and automatically identify these hardware security anomalies.",
      "paperUrl": "https://doi.org/10.1145/3795509",
      "sourceUrl": "",
      "tags": [
        "C++",
        "IR",
        "Security"
      ]
    },
    {
      "id": "openalex-w7125878739",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Conduit: Programmer-Transparent Near-Data Processing Using Multiple Compute-Capable Resources in Solid State Drives",
      "authors": [
        {
          "name": "Rakesh Nadig",
          "affiliation": ""
        },
        {
          "name": "Vamanan Arulchelvan",
          "affiliation": ""
        },
        {
          "name": "Mayank Kabra",
          "affiliation": ""
        },
        {
          "name": "Harshita Gupta",
          "affiliation": ""
        },
        {
          "name": "Rahul Bera",
          "affiliation": ""
        },
        {
          "name": "Nika Mansouri Ghiasi",
          "affiliation": ""
        },
        {
          "name": "Nanditha Rao",
          "affiliation": ""
        },
        {
          "name": "Qingcai Jiang",
          "affiliation": ""
        },
        {
          "name": "Andreas Kosmas Kakolyris",
          "affiliation": ""
        },
        {
          "name": "Yu Liang",
          "affiliation": ""
        },
        {
          "name": "Mohammad Sadrosadati",
          "affiliation": ""
        },
        {
          "name": "Onur Mutlu",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "publication": "ArXiv.org",
      "venue": "ArXiv.org",
      "type": "research-paper",
      "abstract": "Solid-state drives (SSDs) are well suited for near-data processing (NDP) because they: (1) store large application datasets, and (2) support three NDP paradigms: in-storage processing (ISP), processing using DRAM in the SSD (PuD-SSD), and in-flash processing (IFP). A large body of prior SSD-based NDP techniques operate in isolation, mapping computations to only one or two NDP paradigms (i.e., ISP, PuD-SSD, or IFP) within the SSD. These techniques (1) are tailored to specific workloads or kernels, (2) do not exploit the full computational potential of an SSD, and (3) lack programmer-transparency. While several prior works propose techniques to partition computation between the host and near-memory accelerators, adapting these techniques to SSDs has limited benefits because they (1) ignore the heterogeneity of the SSD resources, and (2) make offloading decisions based on limited factors such as bandwidth utilization, or data movement cost. We propose Conduit, a general-purpose, programmer-transparent NDP framework for SSDs that leverages multiple SSD computation resources. At compile time, Conduit executes a custom compiler (e.g., LLVM) pass that (i) vectorizes suitable application code segments into SIMD operations that align with the SSD's page layout, and (ii) embeds metadata (e.g., operation type, operand sizes) into the vectorized instructions to guide runtime offloading decisions. At runtime, within the SSD, Conduit performs instruction-granularity offloading by evaluating six key features, and uses a cost function to select the most suitable SSD resource. We evaluate Conduit and two prior NDP offloading techniques using an in-house event-driven SSD simulator on six data-intensive workloads. Conduit outperforms the best-performing prior offloading policy by 1.8x and reduces energy consumption by 46%.",
      "paperUrl": "https://arxiv.org/pdf/2601.17633",
      "sourceUrl": "http://arxiv.org/abs/2601.17633",
      "tags": []
    },
    {
      "id": "openalex-w7127545678",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Compilation de contremesures contre des attaques en faute avec « Tracing LLVM »",
      "authors": [
        {
          "name": "Sébastien Michelland",
          "affiliation": ""
        },
        {
          "name": "Christophe Deleuze",
          "affiliation": ""
        },
        {
          "name": "Laure Gonnord",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "publication": "SPIRE - Sciences Po Institutional REpository",
      "venue": "SPIRE - Sciences Po Institutional REpository",
      "type": "research-paper",
      "abstract": "International audience",
      "paperUrl": "https://hal.science/hal-05483310",
      "sourceUrl": "",
      "tags": []
    },
    {
      "id": "openalex-w7127132104",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Building a multi-arch CI pipeline for 13 targets. What could possibly go wrong?",
      "authors": [
        {
          "name": "Marek Pikuła",
          "affiliation": "Samsung (Poland)"
        }
      ],
      "year": "2026",
      "publication": "Zenodo (CERN European Organization for Nuclear Research)",
      "venue": "Zenodo (CERN European Organization for Nuclear Research)",
      "type": "research-paper",
      "abstract": "The ci-multiplatform project is a generic, OCI-based multi-architecture CI system designed to make cross-platform testing practical for open-source projects using GitLab CI. Originally created while enabling RISC-V support for Pixman, it has since grown into an independent project under the RISE (RISC-V Software Ecosystem) umbrella: https://gitlab.com/riseproject/CI/ci-multiplatform, with a mirror on freedesktop.org: https://gitlab.freedesktop.org/pixman/ci-multiplatform The project provides multi-arch layered OCI images (Base GNU, LLVM, Meson) based on Debian, GitLab Component-style templates, and fully automated downstream test pipelines for the included examples. It supports creating customized OCI images, building and testing across 16 Linux and Windows targets – including x86, ARM, RISC-V, MIPS, and PowerPC – using unprivileged GitLab runners with QEMU user-mode emulation. Architecture-specific options (e.g., RISC-V VLEN configuration) allow developers to exercise multiple virtual hardware profiles without any physical hardware, all within a convenient job-matrix workflow. The talk covers how the system is engineered, tested, and validated across multiple GitLab instances, and what happens when unprivileged runners, QEMU quirks, toolchain differences, and architecture-specific behaviours all converge in a single pipeline. I will show how projects can adopt ci-multiplatform with minimal effort and turn multi-arch CI from a maintenance burden into a routine part of upstream development.",
      "paperUrl": "https://doi.org/10.5281/zenodo.18457396",
      "sourceUrl": "",
      "tags": [
        "Testing"
      ]
    },
    {
      "id": "openalex-w7129084048",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "BONC: a framework for automatic cryptanalysis using the implementation code",
      "authors": [
        {
          "name": "Yu Gu",
          "affiliation": "Institute of Information Engineering"
        },
        {
          "name": "Meicheng Liu",
          "affiliation": "Institute of Information Engineering"
        }
      ],
      "year": "2026",
      "publication": "Cybersecurity",
      "venue": "Cybersecurity | Vol. 9 (Issue 1)",
      "type": "research-paper",
      "abstract": "Abstract The security analysis of symmetric ciphers is a time-consuming and labor-intensive process that traditionally relies on manual derivation and mathematical modeling for specific algorithms. This paper introduces a fully automated analysis software framework designed to evaluate the security of any round-based symmetric cipher constructed from common primitives like S-boxes, bit permutations, and XOR operations. Our approach leverages the LLVM compiler infrastructure and the KLEE symbolic execution engine to automatically convert C implementations of cryptographic algorithms into a structured representation of state bits and update functions. By employing a frontend that uses custom annotations to identify round functions, it translates the algorithm into a series of Bit Expressions, and we introduce multiple modular backends that process those Expressions to applicable form of performing various types of analyzing methods. We demonstrate the tool’s effectiveness by implementing a differential and linear cryptanalysis backend with help of SAT solver, Division Property integral cryptanalysis backend with help of MILP solver, and a degree estimation backend based on Numeric Mapping method. As an experimental result, we apply these automated analysis techniques to several finalists from the NIST Lightweight Cryptography (LWC) competition, successfully reproducing and improving some existing cryptanalytic results. This work significantly lowers the barrier for cryptographic research by providing a powerful and adaptable platform for automatically assessing the security of both existing and newly designed symmetric ciphers.",
      "paperUrl": "https://link.springer.com/content/pdf/10.1186/s42400-026-00556-9.pdf",
      "sourceUrl": "https://doi.org/10.1186/s42400-026-00556-9",
      "tags": [
        "Backend",
        "Frontend",
        "Infrastructure",
        "Security"
      ]
    },
    {
      "id": "openalex-w7118996119",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Assessment of Multicore Processor Soft Error Reliability Using BBRO ‐ DNN and SSF ‐ FIS Models",
      "authors": [
        {
          "name": "Usha Jadhav",
          "affiliation": "Dr. D. Y. Patil Medical College, Hospital and Research Centre"
        },
        {
          "name": "P. Malathi",
          "affiliation": "Savitribai Phule Pune University"
        }
      ],
      "year": "2026",
      "publication": "Concurrency and Computation Practice and Experience",
      "venue": "Concurrency and Computation Practice and Experience | Vol. 38 (Issue 1)",
      "type": "research-paper",
      "abstract": "ABSTRACT The development of virtual platform frameworks has made it possible to perform early soft error analysis of more realistic multicore systems, that is, real software stacks and state‐of‐the‐art ISAs. Because of the underlying frameworks' strong observability and simulation performance, more error/failure related data may be generated and collected in a reasonable amount of time even with complicated software stack setups. Parameters (i.e., features) that do not directly connect to the system soft error analysis must be filtered away when working with sizable failure‐related data sets that come from several fault campaigns. In this regard, the paper proposes an assessment of multicore processor soft error reliability using BBRO‐DNN and SSF‐FIS models. At first, source code is converted into the executable code using LLVM compiler and applied over the Gem 5 virtual platform. Then, faults are injected into the fault injection module of the virtual platform. Profiling module analysis the faults and the reaction of the system and submits the report. The fault report is given into the proposed BBRO‐DNN model for classifying the fault type. Finally, the system's reliability is evaluated using classified fault type. Experimental results are done by comparing the proposed and existing models to show the superiority of the developed model.",
      "paperUrl": "https://doi.org/10.1002/cpe.70525",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ]
    },
    {
      "id": "openalex-w7128564916",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "An extension of C++ with memory-centric specifications for HPC to reduce memory footprints and streamline MPI development",
      "authors": [
        {
          "name": "Tobias Weinzierl",
          "affiliation": ""
        },
        {
          "name": "Pawel K. Radtke",
          "affiliation": ""
        },
        {
          "name": "Christian Barrera-Hinojosa",
          "affiliation": ""
        },
        {
          "name": "Mladen Ivkovic",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "publication": "Durham Research Online (Durham University)",
      "venue": "Durham Research Online (Durham University)",
      "type": "research-paper",
      "abstract": "C++ leans towards a memory-inefficient storage of structs:The compiler inserts padding bits, while it is not able to exploit knowledge about the range of integers, enums or bitsets.Furthermore, the language provides no support for arbitrary floating-point precisions.We propose a language extension based upon attributes through whichdevelopers can guide the compiler what memory arrangements would be beneficial:Can multiple booleans or integers with limited range be squeezed into one bit field, do floating-point numbers hold fewer significant bits than in the IEEE standard, and is a programmer willing to trade attribute ordering guarantees for a more compact object representation?The extension offers the opportunity tofall back to normal alignment and native C++ floating point representations via plain C++ assignments,no dependencies upon external libraries are introduced, and the resulting coderemains (syntactically) standard C++.As MPI remains the de-facto standard for distributed memory calculations in C++,we furthermore propose additional attributes which streamline the MPI datatype modelling in combination with our memory optimisation extensions.Our work implements the language annotations within LLVM and demonstrates their potential impact through smoothed particle hydrodynamics benchmarks.They uncover the potential gains in terms of performance and development productivity.",
      "paperUrl": "https://durham-repository.worktribe.com/output/5079686",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Libraries",
        "Performance"
      ]
    },
    {
      "id": "openalex-w7117345953",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "ВИКОРИСТАННЯ ГЕНЕТИЧНИХ АЛГОРИТМІВ В АДАПТИВНИХ КОМПІЛЯТОРАХ ДЛЯ КРОСПЛАТФОРМЕНОЇ ОПТИМІЗАЦІЇ",
      "authors": [
        {
          "name": "М. Г. Бердник",
          "affiliation": ""
        },
        {
          "name": "І. П. Стародубський",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "publication": "The Scientific Issues of Ternopil Volodymyr Hnatiuk National Pedagogical University Series pedagogy",
      "venue": "The Scientific Issues of Ternopil Volodymyr Hnatiuk National Pedagogical University Series pedagogy",
      "type": "research-paper",
      "abstract": "Context. Modern software is developed under conditions of continuously increasing complexity of computing systems. Today, developers must consider a vast diversity of platforms, ranging from resource-constrained mobile devices to servers with highperformance processors and specialized architectures such as GPUs, FPGAs and even quantum computers. This heterogeneity requires software to operate efficiently across various hardware platforms. However, portability remains one of the most challenging tasks, especially when high performance is required. One of the promising directions to address this problem is the use of adaptive compilers that can automatically optimize code for different architectures. This approach allows developers to focus on the functional part of the software, minimizing the effort spent on optimization and configuration. Genetic algorithms (GAs) play a special role among the methods used to create adaptive compilers. These are powerful evolutionary techniques that allow finding optimal solutions in complex and multidimensional parameter spaces.Objective. The objective of this research is to apply genetic algorithms in the process of adaptive compilation to enable automatic optimization of software across different hardware platforms.Method. The approach is based on genetic algorithms to automate the compilation process. The key stages include: population initialization – creation of an initial set of compilation parameters; fitness function evaluation – assessment of the efficiency of each parameter combination; evolutionary operations – applying crossover, mutation and selection to generate the next generation of parameters; termination criteria – stopping the iterative process upon reaching an optimal result or stabilization of metrics.Results. The developed algorithm was implemented in Python using the numpy, multiprocessing and subprocess libraries.Performance evaluation of the algorithm was carried out using execution time, energy consumption and memory usage metrics.Conclusions. The scientific novelty of the study lies in the development of: an innovative approach to automatic compilation parameter optimization based on genetic algorithms; a method for dynamic selection of optimization strategies based on performance metrics for different architectures; integration of GAs with modern compilers such as LLVM for automatic analysis of intermediate representation and code optimization; and methods for applying adaptive compilers to solve cross-platform optimization problems. The practical significance is determined by the use of genetic algorithms in adaptive compilers, which significantly improves the efficiency of the compilation process by automating the selection of optimal parameters for various architectures. The proposed approach can be successfully applied in the fields of mobile computing, cloud technologies and high-performance systems.",
      "paperUrl": "https://ric.zp.edu.ua/article/download/346565/333573",
      "sourceUrl": "https://ric.zp.edu.ua/article/view/346565",
      "tags": [
        "Libraries",
        "Performance"
      ]
    },
    {
      "id": "openalex-w7117165792",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "USE OF GENETIC ALGORITHMS IN ADAPTIVE COMPILERS FOR CROSS-PLATFORM OPTIMIZATION",
      "authors": [
        {
          "name": "M. G. Berdnyk",
          "affiliation": "Dnipro University of Technology"
        },
        {
          "name": "I. P. Starodubskyi",
          "affiliation": "Dnipro University of Technology"
        }
      ],
      "year": "2025",
      "publication": "Radio Electronics Computer Science Control",
      "venue": "Radio Electronics Computer Science Control | Issue 4",
      "type": "research-paper",
      "abstract": "Context. Modern software is developed under conditions of continuously increasing complexity of computing systems. Today, developers must consider a vast diversity of platforms, ranging from resource-constrained mobile devices to servers with highperformance processors and specialized architectures such as GPUs, FPGAs and even quantum computers. This heterogeneity requires software to operate efficiently across various hardware platforms. However, portability remains one of the most challenging tasks, especially when high performance is required. One of the promising directions to address this problem is the use of adaptive compilers that can automatically optimize code for different architectures. This approach allows developers to focus on the functional part of the software, minimizing the effort spent on optimization and configuration. Genetic algorithms (GAs) play a special role among the methods used to create adaptive compilers. These are powerful evolutionary techniques that allow finding optimal solutions in complex and multidimensional parameter spaces.Objective. The objective of this research is to apply genetic algorithms in the process of adaptive compilation to enable automatic optimization of software across different hardware platforms.Method. The approach is based on genetic algorithms to automate the compilation process. The key stages include: population initialization – creation of an initial set of compilation parameters; fitness function evaluation – assessment of the efficiency of each parameter combination; evolutionary operations – applying crossover, mutation and selection to generate the next generation of parameters; termination criteria – stopping the iterative process upon reaching an optimal result or stabilization of metrics.Results. The developed algorithm was implemented in Python using the numpy, multiprocessing and subprocess libraries.Performance evaluation of the algorithm was carried out using execution time, energy consumption and memory usage metrics.Conclusions. The scientific novelty of the study lies in the development of: an innovative approach to automatic compilation parameter optimization based on genetic algorithms; a method for dynamic selection of optimization strategies based on performance metrics for different architectures; integration of GAs with modern compilers such as LLVM for automatic analysis of intermediate representation and code optimization; and methods for applying adaptive compilers to solve cross-platform optimization problems. The practical significance is determined by the use of genetic algorithms in adaptive compilers, which significantly improves the efficiency of the compilation process by automating the selection of optimal parameters for various architectures. The proposed approach can be successfully applied in the fields of mobile computing, cloud technologies and high-performance systems.",
      "paperUrl": "https://doi.org/10.15588/1607-3274-2025-4-16",
      "sourceUrl": "",
      "tags": [
        "Libraries",
        "Performance"
      ]
    },
    {
      "id": "openalex-w4416004095",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Towards Supporting QIR",
      "authors": [
        {
          "name": "Yannick Stade",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Lukas Burgholzer",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Robert Wille",
          "affiliation": "Technical University of Munich"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "Intermediate representations (IRs) play a crucial role in the software stack of a quantum computer to facilitate efficient optimizations for executing an application on hardware. One of those IRs is the Quantum Intermediate Representation (QIR), which builds on the classical LLVM compiler infrastructure. In this article, we outline different approaches to how QIR can be adopted. This exploration culminates in a demonstration of what it takes to turn an existing quantum circuit simulator into a QIR runtime and that such a transition is less daunting than it might seem at first. We further show that switching to QIR does not entail any performance deficits compared to the original simulator. On the contrary, the presented steps effortlessly allow adding support for arbitrary classical control flow to any classical simulator. We conclude with an outlook on future directions using QIR. The implemented QIR runtime is available under https://github.com/munich-quantum-toolkit/core.",
      "paperUrl": "https://doi.org/10.1145/3731599.3767546",
      "sourceUrl": "",
      "tags": [
        "Infrastructure",
        "Optimizations",
        "Performance"
      ]
    },
    {
      "id": "openalex-w7109457649",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Targeted Testing of Compiler Optimizations via Grammar-Level Composition Styles",
      "authors": [
        {
          "name": "Zhou, Zitong",
          "affiliation": ""
        },
        {
          "name": "Limpanukorn, Ben",
          "affiliation": ""
        },
        {
          "name": "Kang, Hong Jin",
          "affiliation": ""
        },
        {
          "name": "Wang, Jiyuan",
          "affiliation": ""
        },
        {
          "name": "Wu, Yaoxuan",
          "affiliation": ""
        },
        {
          "name": "Kiss, Akos",
          "affiliation": ""
        },
        {
          "name": "Hodovan, Renata",
          "affiliation": ""
        },
        {
          "name": "Kim, Miryung",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "publication": "ArXiv.org",
      "venue": "ArXiv.org",
      "type": "research-paper",
      "abstract": "Ensuring the correctness of compiler optimizations is critical, but existing fuzzers struggle to test optimizations effectively. First, most fuzzers use optimization pipelines (heuristics-based, fixed sequences of passes) as their harness. The phase-ordering problem can enable or preempt transformations, so pipelines inevitably miss optimization interactions; moreover, many optimizations are not scheduled, even at aggressive levels. Second, optimizations typically fire only when inputs satisfy specific structural relationships, which existing generators and mutations struggle to produce. We propose targeted fuzzing of individual optimizations to complement pipeline-based testing. Our key idea is to exploit composition styles - structural relations over program constructs (adjacency, nesting, repetition, ordering) - that optimizations look for. We build a general-purpose, grammar-based mutational fuzzer, TargetFuzz, that (i) mines composition styles from an optimization-relevant corpus, then (ii) rebuilds them inside different contexts offered by a larger, generic corpus via synthesized mutations to test variations of optimization logic. TargetFuzz is adaptable to a new programming language by lightweight, grammar-based, construct annotations - and it automatically synthesizes mutators and crossovers to rebuild composition styles. No need for hand-coded generators or language-specific mutators, which is particularly useful for modular frameworks such as MLIR, whose dialect-based, rapidly evolving ecosystem makes optimizations difficult to fuzz. Our evaluation on LLVM and MLIR shows that TargetFuzz improves coverage by 8% and 11% and triggers optimizations 2.8$\\times$ and 2.6$\\times$, compared to baseline fuzzers under the targeted fuzzing mode. We show that targeted fuzzing is complementary: it effectively tests all 37 sampled LLVM optimizations, while pipeline-fuzzing missed 12.",
      "paperUrl": "https://arxiv.org/pdf/2512.04344",
      "sourceUrl": "http://arxiv.org/abs/2512.04344",
      "tags": [
        "MLIR",
        "Optimizations",
        "Testing"
      ]
    },
    {
      "id": "openalex-w7111164020",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "TOWARDS AUTONOMOUS CODE OPTIMIZATION: A REINFORCEMENT LEARNING FRAMEWORK FOR COMPILER DESIGN",
      "authors": [
        {
          "name": "S.Venkatesan",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "publication": "International Journal of Apllied Mathematics",
      "venue": "International Journal of Apllied Mathematics | Vol. 38 (Issue 11s)",
      "type": "research-paper",
      "abstract": "Modern compiler optimization remains one of the most challenging problems in computer systems research. Conventional compiler pipelines rely on static, manually crafted heuristics to determine optimization passes, instruction scheduling, and register allocation. However, as software complexity and hardware heterogeneity increase, these heuristics struggle to generalize across workloads, architectures, and programming paradigms. This paper proposes an autonomous reinforcement learning (RL) framework for compiler design, in which optimization pass selection and parameter tuning are treated as sequential decision-making tasks. The proposed system formulates compiler optimization as a Markov Decision Process (MDP), where the state represents the intermediate representation (IR) of code, the actions correspond to possible optimization passes, and the reward is derived from performance improvements such as reduced execution time or binary size. A Graph Neural Network (GNN) encoder captures structural information from IR graphs, while a deep reinforcement learning agent (e.g., PPO or DQN) learns optimization policies that generalize across programs and architectures. The framework integrates with the LLVM and MLIR compiler infrastructures and is evaluated on benchmark suites including SPEC CPU2017 and PolyBench. Experimental results indicate up to 35% performance improvement over standard -O3 optimization levels and 20% reduction in code size without compromising compilation time. Ablation studies confirm that GNN-based state encoding and multi-objective reward shaping are essential to policy stability and cross-architecture generalization. This study contributes a modular, scalable approach to autonomous code optimization, bridging the gap between classical compiler theory and data-driven decision systems. The paper concludes with open challenges in interpretability, real-time adaptation, and integration with differentiable compiler toolchains.",
      "paperUrl": "https://ijamjournal.org/ijam/publication/index.php/ijam/article/download/1286/1173",
      "sourceUrl": "https://doi.org/10.12732/ijam.v38i11s.1286",
      "tags": [
        "Infrastructure",
        "IR",
        "MLIR",
        "Performance"
      ]
    },
    {
      "id": "openalex-w4416557631",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Speculative Recursion Unrolling",
      "authors": [
        {
          "name": "Tim Heldmann",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Tim Ziegler",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Peter Arzt",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Christian Bischof",
          "affiliation": "Technical University of Darmstadt"
        }
      ],
      "year": "2025",
      "publication": "Lecture notes in computer science",
      "venue": "Lecture notes in computer science",
      "type": "research-paper",
      "abstract": "No abstract available in OpenAlex metadata.",
      "paperUrl": "https://doi.org/10.1007/978-3-032-07612-0_23",
      "sourceUrl": "",
      "tags": []
    },
    {
      "id": "openalex-w7117174784",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems",
      "authors": [
        {
          "name": "Prathamesh Devadiga",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "publication": "ArXiv.org",
      "venue": "ArXiv.org",
      "type": "research-paper",
      "abstract": "Traditional auto-parallelizing compilers, reliant on rigid heuristics, struggle with the complexity of modern heterogeneous systems. This paper presents a comprehensive evaluation of small (approximately 1B parameter) language-model-driven compiler auto-parallelization. We evaluate three models: gemma3, llama3.2, and qwen2.5, using six reasoning strategies across 11 real-world kernels drawn from scientific computing, graph algorithms, and machine learning. Our system is benchmarked against strong compiler baselines, including LLVM Polly, TVM, and Triton. Across 376 total evaluations, the proposed approach achieves an average speedup of 6.81x and a peak performance of 43.25x on convolution operations. We analyze scalability, verify correctness using multiple sanitizers, and confirm robustness across diverse compilers and hardware platforms. Our results demonstrate that small, efficient language models can serve as powerful reasoning engines for complex compiler optimization tasks.",
      "paperUrl": "https://arxiv.org/pdf/2512.19250",
      "sourceUrl": "http://arxiv.org/abs/2512.19250",
      "tags": [
        "Performance",
        "Polly"
      ]
    },
    {
      "id": "openalex-w7118196566",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Session Summary Podcast: Session 16: LLVM-HPC: The Eleventh Annual Workshop on the LLVM Compiler Infrastructure in HPC (LLVM-HPC’25)",
      "authors": [
        {
          "name": "AI Generated",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "No abstract available in OpenAlex metadata.",
      "paperUrl": "https://doi.org/10.1145/3731599.3787537",
      "sourceUrl": "",
      "tags": [
        "Infrastructure"
      ]
    },
    {
      "id": "openalex-w7129194239",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "SLIM: A Simplified LLVM IR Abstraction for Pointer Analysis",
      "authors": [
        {
          "name": "Aditi Raste",
          "affiliation": "Savitribai Phule Pune University"
        },
        {
          "name": "Swati Jaiswal",
          "affiliation": "Visvesvaraya National Institute of Technology"
        },
        {
          "name": "Uday P. Khedker",
          "affiliation": "Indian Institute of Technology Bombay"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "Pointer analysis is a static code analysis technique that derives points-to information of the pointers used in a program. This information can be used to enhance the precision and reasoning capability of LLMs when dealing with program semantics, to detect bugs in the program, identify security vulnerabilities, program verification, etc. LLVM (Low-Level Virtual Machine) is a widely used compiler framework that has gained significant popularity in industry as well as in academia for the ease of adding passes to it due to its well-designed IR. However, it is observed that program analysts when performing pointer analysis often encounter significant challenges in understanding the LLVM IR because (a) it employs a low-level, load-storebased memory abstraction instead of the higher-level, namebased abstraction used in three-address code (b) and includes numerous low-level details within its instructions. Hence, there exists a pressing demand, particularly for pointer analysis for an abstraction layer over the LLVM IR that will extract only the relevant information from the IR while ignoring the rest. To address this, we present SLIM (Simplified LLVM IR Modelling), an open-source tool engineered as a high-level abstraction over LLVM IR. SLIM employs a name-based memory abstraction and conceals low-level details, producing a clean intermediate representation that is ideal for pointer analysis. We observe that (a) With SLIM there is a reduction in the instruction count for performing pointer analysis as compared to LLVM IR by an average reduction of 52% on the programs analyzed from SPEC CPU 2017 benchmark suite. (b) SLIM provides an easy framework for implementing an analysis as per the scaffolded study conducted at IIT Bombay and VNIT, Nagpur.",
      "paperUrl": "https://doi.org/10.1109/punecon67554.2025.11377611",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Security"
      ]
    },
    {
      "id": "openalex-w7115599564",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "SLIM: A Simplified LLVM IR Abstraction for Compiler Design Courses",
      "authors": [
        {
          "name": "Aditi Raste",
          "affiliation": "Savitribai Phule Pune University"
        },
        {
          "name": "Swati Jaiswal",
          "affiliation": "Visvesvaraya National Institute of Technology"
        },
        {
          "name": "Uday Khedker",
          "affiliation": "Indian Institute of Technology Bombay"
        }
      ],
      "year": "2025",
      "publication": "Communications in computer and information science",
      "venue": "Communications in computer and information science",
      "type": "research-paper",
      "abstract": "No abstract available in OpenAlex metadata.",
      "paperUrl": "https://doi.org/10.1007/978-3-032-14583-3_15",
      "sourceUrl": "",
      "tags": [
        "IR"
      ]
    },
    {
      "id": "openalex-w7124919900",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "SELF-PROFILING MECHANISMS FOR REAL-TIME CODE COMPILERS",
      "authors": [
        {
          "name": "M. Berdnyk",
          "affiliation": ""
        },
        {
          "name": "I. Starodubskyi",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "publication": "System technologies",
      "venue": "System technologies | Vol. 6 (Issue 161)",
      "type": "research-paper",
      "abstract": "This paper explores the concept of self-profiling compilers as a means of adaptive real-time code optimization. The approach relies on collecting dynamic performance metrics during program execution and analyzing the collected data to choose the most effective compilation strategies. We propose a compiler architecture capable of automatically detecting performance-critical code regions (hotspots), adjusting the configuration of optimization passes, and recompiling code with updated metrics. A prototype was implemented based on LLVM with an embedded runtime agent responsible for code instrumentation, metric collection, and interaction with a dynamic Pass Manager. A series of experiments were conducted across various hardware platforms, including desktop CPUs and ARM-based architectures. The results demonstrated significant performance gains without noticeable increases in compilation time or resource usage. These findings confirm the feasibility of integrating self-profiling into next-generation compilers targeting high-performance computing, edge systems, and mobile devices. The paper presents the concept of self-profiling as a tool for real-time code optimization. A prototype based on LLVM with embedded runtime analysis has been implemented. The results demonstrate the advantages of the proposed approach.",
      "paperUrl": "https://doi.org/10.34185/1562-9945-5-161-2025-08",
      "sourceUrl": "",
      "tags": [
        "Embedded",
        "Performance"
      ]
    },
    {
      "id": "openalex-w7117237469",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "SDF-Fuzz: State-Driven Fragmentation for SSD Firmware Validation",
      "authors": [
        {
          "name": "Anonymous",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "publication": "Zenodo (CERN European Organization for Nuclear Research)",
      "venue": "Zenodo (CERN European Organization for Nuclear Research)",
      "type": "research-paper",
      "abstract": "Data for SDF-Fuzz Overview This record contains the experimental data and source code accompanying the paper “SDF-Fuzz: State-Driven Fragmentation Fuzzer for SSD Firmware Testing”. It includes implementations of the Threshold Variable Discovery Algorithm (TVDA), Dynamic Range Partitioning (DRP), and the FTL Fragmentation Metric (FFM), together with experimental results on FEMU and MQSim platforms. The dataset also provides raw execution logs and LLVM coverage data that were used to evaluate the effectiveness of the proposed framework. File Manifest Result_Overview.xlsx: Comprehensive summary of experimental results comparing SDF-Fuzz with baseline fuzzers (AFL++ and CSFuzz) across FEMU and MQSim environments. TVDA_Variable_Extractor.py: Python implementation of TVDA for identifying GC-triggering variables via static analysis. MQSim_TVDA/_CSFuzz_CVDG.txt and FEMU_TVDA/_CSFuzz_CVDG.txt: Output logs generated by TVDA_Variable_Extractor.py, listing extracted threshold variables and their dependency graphs for each platform. FEMU_TVDA_DRP_FFM.py: AFL++ custom mutator integrating TVDA, DRP, and FFM for FEMU. MQSim_TVDA_DRP.py: AFL++ custom mutator implementing TVDA and DRP for MQSim. FFM_Code_Explanation.txt: Detailed explanation of the structure and logic of FEMU_TVDA_DRP_FFM.py. MQSim_Persistent_Mode_Instrumented_Code_Setup.txt: Setup instructions for running MQSim with AFL++ in persistent mode. Initial_Seed_Test_input: Initial seed file used by the AFL++ custom mutator. CSFuzz_Reproduce_Setup.txt: Instructions for reproducing CSFuzz baseline experiments. MQSim_FEMU_Result.7z (files divided): Archive containing raw fuzzing artifacts, including: Coverage profiles (*.profraw): Raw LLVM coverage data for validating executed code paths. Command logs (nvme_fuzz.log, mqsim.log): Sequences of NVMe commands issued during testing. Fuzzer state logs (*_mutator.log): Traces of internal state transitions and corpus updates during fuzzing. Crash/Hang inputs (data.tar.gz): Inputs stored by AFL++ as potential crash or hang causes (not validated as actual firmware bugs). ConfigurationAll experiments on FEMU (emulator) and MQSim (simulator) were conducted using default SSD configuration parameters, including density, channel count, plane count, die count, and garbage collection (GC) trigger thresholds. System Requirements Operating system: Ubuntu 20.04 LTS or later Python: Version 3.8 or higher (required for running the provided Python scripts)",
      "paperUrl": "https://doi.org/10.5281/zenodo.18033289",
      "sourceUrl": "",
      "tags": [
        "Static Analysis",
        "Testing"
      ]
    },
    {
      "id": "openalex-w7126022191",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Runtime Type-Based Access Control for eBPF",
      "authors": [
        {
          "name": "Leonardo Giovannoni",
          "affiliation": "University of Pisa"
        },
        {
          "name": "Giuseppe Gaetano Lettiéri",
          "affiliation": "University of Pisa"
        },
        {
          "name": "Gregorio Procissi",
          "affiliation": "University of Pisa"
        }
      ],
      "year": "2025",
      "publication": "CINECA IRIS Institutial research information system (University of Pisa)",
      "venue": "CINECA IRIS Institutial research information system (University of Pisa)",
      "type": "research-paper",
      "abstract": "eBPF’s kernel-level flexibility introduces security risks as untrusted programs gain unrestricted access to sensitive data structures. We propose Runtime Type-Based Access Control (RTBAC), a compiler-driven framework that automatically enforces security policies on eBPF programs. By integrating a custom LLVM pass, RTBAC injects runtime checks into the Intermediate Representation (IR) to validate pointer types during dereferencing against kernel-defined policies. This approach leverages Linux’s BPF Type Format (BTF) to resolve nested structures and pointers, while kernel modifications isolate type metadata per program. The solution requires no developer intervention, mitigates risks of third-party eBPF code, and integrates with existing workflows via an out-of-tree LLVM pass.",
      "paperUrl": "https://doi.org/10.1109/nfv-sdn66355.2025.11349638",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Rust",
        "Security"
      ]
    },
    {
      "id": "openalex-w4417452058",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Research on the integration method of link optimization based on basic block reordering in GNU linker",
      "authors": [
        {
          "name": "Yong‐Xiang Chen",
          "affiliation": "Xi’an University of Posts and Telecommunications"
        },
        {
          "name": "Yagang Wang",
          "affiliation": "Xi’an University of Posts and Telecommunications"
        },
        {
          "name": "Shuai Du",
          "affiliation": "Xi’an University of Posts and Telecommunications"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "LLVM has introduced tools such as BOLT and Propeller that perform optimizations like basic block reordering and function reordering on already-linked binaries, significantly improving program performance. Inspired by this, we propose a method that integrates the basic block reordering algorithm from BOLT into the GNU ld linker, enabling link-time optimization based on runtime performance data to overcome the limitations of traditional compile-time optimization. This method replaces the .text section of relocatable object files with optimized data during the linking stage, precisely adjusts jump instruction offsets, and updates key metadata structures such as the relocation and symbol tables. The final output is an optimized executable. Experiments on the SPEC2017 benchmark suite show that this method yields significant performance improvements, with 505.mcf r and 557.xz achieving 2.1% and 1.08% gains respectively. In contrast, 544.nab shows limited improvement, demonstrating both the effectiveness of link-time dynamic optimization and its sensitivity to program characteristics.",
      "paperUrl": "https://doi.org/10.1117/12.3094136",
      "sourceUrl": "",
      "tags": [
        "Optimizations",
        "Performance"
      ]
    },
    {
      "id": "openalex-w7108320809",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Research on SIMD Instruction Sequence Generation Method for Vector DSP Processor",
      "authors": [
        {
          "name": "Yonghua Hu",
          "affiliation": "Hunan University of Science and Technology"
        },
        {
          "name": "Fangjun Liu",
          "affiliation": "Hunan University of Science and Technology"
        },
        {
          "name": "Huifu Zhang",
          "affiliation": "Hunan University of Science and Technology"
        },
        {
          "name": "Ju Huang",
          "affiliation": "Hunan University of Science and Technology"
        },
        {
          "name": "Hao Chen",
          "affiliation": "Hunan University of Science and Technology"
        }
      ],
      "year": "2025",
      "publication": "Concurrency and Computation Practice and Experience",
      "venue": "Concurrency and Computation Practice and Experience | Vol. 38 (Issue 1)",
      "type": "research-paper",
      "abstract": "ABSTRACT In the field of digital signal processing (DSP), the execution of vector operations depends on the optimization of Single Instruction Multiple Data (SIMD) technology. However, manual SIMD vectorization method is complex to develop, poorly portable, and costly to maintain. Therefore, we propose a method of SIMD instruction sequence generation based on LLVM. This method builds a hierarchical instruction generation framework, combines the characteristics of the target architecture, and uses LLVM automatic vectorization tool to gradually convert the vectorized intermediate representation into the target architecture instruction sequence containing SIMD instructions. Experiments on FT‐M7002 hardware platform show that, compared with the vectorization method of manually calling SIMD built‐in functions, the average execution performance of the instruction sequence generated by this method can be improved by up to 70%.",
      "paperUrl": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cpe.70377",
      "sourceUrl": "https://doi.org/10.1002/cpe.70377",
      "tags": [
        "Performance"
      ]
    },
    {
      "id": "openalex-w7127095517",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "RL-Optimized Lightweight Obfuscation Against Binary Code Similarity Detection",
      "authors": [
        {
          "name": "Ran Wei",
          "affiliation": "Ministry of Education"
        },
        {
          "name": "Hui Shu",
          "affiliation": "Ministry of Education"
        },
        {
          "name": "Fei Kang",
          "affiliation": "Ministry of Education"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "Binary Code Similarity Detection (BCSD) identifies functions with similar functionality across binaries. Reverse engineers leverage BCSD techniques to locate critical functions in software, thereby compromising associated functional modules. This poses significant challenges for software protection. Although code obfuscation can counter BCSD, existing methods typically apply global, undifferentiated obfuscation, leading to unnecessary code size and runtime overhead. This paper proposes ReinObf, a reinforcement learning-optimized method for lightweight and differentiated obfuscation. In the LLVM intermediate representation of functions, we implement a custom pass to extract statistical and structural features from basic blocks and construct Attributed Control Flow Graphs (ACFGs). A hierarchical RL agent encodes ACFGs using graph neural networks, selects critical blocks and optimal obfuscation methods, then iteratively generates optimized obfuscation sequences that maximize evasion effectiveness while minimizing overhead. Experimental validation demonstrates that ReinObf effectively evades BinDiff, jTrans, CLAP, and Gemini, with an average similarity of 0.391. Compared to OLLVM’s combined obfuscation, it reduces code size overhead by 66.3% and runtime overhead by 34.7%, achieving a favorable security-performance tradeoff.",
      "paperUrl": "https://doi.org/10.1109/trustcom66490.2025.00096",
      "sourceUrl": "",
      "tags": [
        "Performance",
        "Security"
      ]
    },
    {
      "id": "openalex-w4416004079",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "OpenSHMEM MLIR: A Dialect for Compile-Time Optimization of One-Sided Communications",
      "authors": [
        {
          "name": "Michael Beebe",
          "affiliation": "Texas Tech University"
        },
        {
          "name": "B Michałowicz",
          "affiliation": "The Ohio State University"
        },
        {
          "name": "Andrew McNamara",
          "affiliation": "New Mexico Consortium"
        },
        {
          "name": "Y V Ravi Kumar",
          "affiliation": "Texas Tech University"
        },
        {
          "name": "Dhabaleswar K. Panda",
          "affiliation": "The Ohio State University"
        },
        {
          "name": "Yong Chen",
          "affiliation": "Texas Tech University"
        },
        {
          "name": "Wendy Poole",
          "affiliation": "Los Alamos National Laboratory"
        },
        {
          "name": "Stephen Poole",
          "affiliation": "Los Alamos National Laboratory"
        }
      ],
      "year": "2025",
      "publication": "ThinkTech (Texas Tech University)",
      "venue": "ThinkTech (Texas Tech University)",
      "type": "research-paper",
      "abstract": "Communication increasingly limits performance in high-performance computing (HPC), yet mainstream compilers focus on computation because communication intent is lost early in compilation. OpenSHMEM offers a one-sided Partitioned Global Address Space (PGAS) model with symmetric memory and explicit synchronization, but lowering to opaque runtime calls hides these semantics from analysis. We present an OpenSHMEM dialect for Multi-Level Intermediate Representation (MLIR) that preserves one-sided communication, symmetric memory, and team/context structure as first-class intermediate representation (IR) constructs. Retaining these semantics prior to lowering enables precise, correctness-preserving optimizations that are difficult to recover from LLVM IR. The dialect integrates with existing MLIR/LLVM passes while directly representing communication and synchronization intent. We demonstrate four transformations: recording the number of processing elements, fusing compatible atomics, converting blocking operations to non-blocking forms when safe, and aggregating small messages. These examples show how explicit OpenSHMEM semantics enable communication-aware optimization and lay the groundwork for richer cross-layer analyses.",
      "paperUrl": "https://doi.org/10.1145/3731599.3767483",
      "sourceUrl": "",
      "tags": [
        "IR",
        "MLIR",
        "Optimizations",
        "Performance"
      ]
    },
    {
      "id": "openalex-w4416203904",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "ODOS-MPI: HPC-Friendly SmartNIC Offloading of Computation/Communication Kernels",
      "authors": [
        {
          "name": "Muhammad Usman",
          "affiliation": "Barcelona Supercomputing Center"
        },
        {
          "name": "Mariano Benito",
          "affiliation": "Universitat Politècnica de Catalunya"
        },
        {
          "name": "Sergio Iserte",
          "affiliation": "Barcelona Supercomputing Center"
        },
        {
          "name": "Antonio J. Peña",
          "affiliation": "Universitat Politècnica de Catalunya"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "The increasing complexity and scale of high-performance computing (HPC) workloads demand innovative approaches to optimize both computation and communication. While OpenMP has been widely adopted for intra-node parallelism and MPI for inter-node communication, emerging SmartNICs introduce new opportunities for offloading communication-intensive tasks. In this work, we extend OpenMP to support MPI kernel offloading to SmartNICs. Our implementation integrates Open MPI communication offloading into the LLVM compiler while utilizing DOCA SDK for efficient interaction with Nvidia BlueField DPUs. Leveraging OpenMP eliminates the need for direct low-level programming, lowering the entry barrier for domain scientists. We demonstrate our framework’s versatility by implementing a SmartNIC-enabled version of the MPI OSU micro-benchmarks and improving the execution time of an atmospheric weather simulation by over 18%, thanks to concurrent computation and communication.",
      "paperUrl": "https://doi.org/10.1145/3712285.3759808",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ]
    },
    {
      "id": "openalex-w7115591921",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Nugget: Portable Program Snippets (Artifact Evaluation - Reproduction)",
      "authors": [
        {
          "name": "Qiu, Zhantong",
          "affiliation": "University of California, Davis"
        },
        {
          "name": "Samani, Mahyar",
          "affiliation": "University of California, Davis"
        },
        {
          "name": "Lowe-Power, Jason",
          "affiliation": "Lawrence Berkeley National Laboratory"
        }
      ],
      "year": "2025",
      "publication": "Zenodo (CERN European Organization for Nuclear Research)",
      "venue": "Zenodo (CERN European Organization for Nuclear Research)",
      "type": "research-paper",
      "abstract": "Evaluating architectural ideas on realistic workloads is increasingly challenging due to the prohibitive cost of detailed simulation and the lack of portable sampling tools. Existing targeted sampling techniques are often tied to specific binaries, incur significant overhead, and make rapid validation across systems infeasible. To address these limitations, we introduce Nugget, a flexible framework that enables portable sampling across simulators, hardware, architectural differences, and libraries. Nugget leverages LLVM IR to perform binary-independent interval analysis, then generates lightweight, cross-platform executable snippets (nuggets), that can be validated natively on real hardware before use in simulation. This approach decouples samples from specific binaries, dramatically reduces analysis overhead, and allows researchers to iterate on sampling methodologies while efficiently validating samples across diverse systems.Latest version created at Release v1.0.0 · darchr/nugget-AE.",
      "paperUrl": "https://doi.org/10.5281/zenodo.17934861",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Libraries"
      ]
    },
    {
      "id": "openalex-w7117741326",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Modern Llvm-Based Compiler Autotuning for Wcet Optimization",
      "authors": [
        {
          "name": "Gabriele Magnani",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Davide Baroffio",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Federico Reghenzani",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Giovanni Agosta",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Fornaciari William",
          "affiliation": "Politecnico di Milano"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "The problem of compiler optimization selection and ordering, known in the literature as compiler autotuning, has been tackled many times for average-case execution time reduction. Optimizing the WCET is becoming a prominent problem for modern hard real-time systems, where the difficulties in accurate WCET estimation hinder the full exploitation of computing platform capabilities. In this article, we propose a novel methodology and a tool based on LLVM for iterative WCET-driven compiler autotuning, which is the first strategy to operate at function-level granularity and to consider not only the selection of optimization passes, but also their ordering. Our findings show that standard optimization levels <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathrm{O} 0, \\mathrm{O} 1, \\mathrm{O} 2$</tex>, and O 3 are suboptimal when targeting the WCET, and that a per-function selection and ordering of the transformations is necessary. Experimental results show that our approach outperforms the standard optimizations and opens up new directions for future research.",
      "paperUrl": "https://doi.org/10.1109/rtss66672.2025.00043",
      "sourceUrl": "",
      "tags": [
        "Optimizations"
      ]
    },
    {
      "id": "openalex-w7127156588",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "MLSBOX: Automated Sandbox for Fine-Grained Isolation of Multiple Third-Party Libraries *",
      "authors": [
        {
          "name": "Yuqi Qiu",
          "affiliation": "Wuhan University"
        },
        {
          "name": "Yì Wáng",
          "affiliation": "Wuhan University"
        },
        {
          "name": "Chenjun Ma",
          "affiliation": "Wuhan University"
        },
        {
          "name": "Yunfeng Kang",
          "affiliation": "Huawei Technologies (China)"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "In the development process, utilizing the existing functions and interfaces of third-party libraries can significantly reduce development time and resources. However, the introduction of third-party libraries also brings security risks. These libraries may contain defects or vulnerabilities and can even conceal malicious code. Introducing vulnerable third-party libraries can compromise the security of the entire software system. Although existing solutions can provide sandbox environments for libraries, they require extensive modifications to the source code to implement isolation. Furthermore, current sandboxes cannot support the simultaneous isolation of multiple libraries, including memory and privilege isolation. To address these issues, we propose MLSBOX, the first automated sandboxing framework designed to provide fine-grained isolation for multiple untrusted third-party libraries. This sandbox leverages Attribute Program Dependence Graph (APDG) analysis to identify data dependencies from different libraries. It automatically modifies the program at the compiler level to partition the program into multiple independent isolation domains, enforcing memory and privilege isolation for each domain to enhance overall program security. We have successfully implemented a prototype of MLSBOX on LLVM and conducted rigorous functional and performance evaluations on third-party libraries such as OpenSSL and Zlib. MLSBOX provides the most fine-grained memory and permission isolation currently available. In terms of runtime efficiency, MLSBOX incurs an average overhead of 2.50%. For isolating multi-threaded programs, such as a program with 8 threads, MLSBOX incurs a minimal runtime overhead of just 0.80%, significantly lower than that of the current state-of-the-art solution, Cali (365.70%).",
      "paperUrl": "https://doi.org/10.1109/trustcom66490.2025.00095",
      "sourceUrl": "",
      "tags": [
        "Libraries",
        "Performance",
        "Rust",
        "Security"
      ]
    },
    {
      "id": "openalex-w4416004087",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Lowering and Runtime Support for Fortran’s Multi-Image Parallel Features using LLVM Flang, PRIF, and Caffeine",
      "authors": [
        {
          "name": "Dan Bonachea",
          "affiliation": "Lawrence Berkeley National Laboratory"
        },
        {
          "name": "Katherine Rasmussen",
          "affiliation": "Lawrence Berkeley National Laboratory"
        },
        {
          "name": "Damian Rouson",
          "affiliation": "Lawrence Berkeley National Laboratory"
        },
        {
          "name": "Jean-Didier Pailleux",
          "affiliation": "Maison des Sciences de l'Homme"
        },
        {
          "name": "Étienne Renault",
          "affiliation": "Maison des Sciences de l'Homme"
        },
        {
          "name": "Brad Richardson",
          "affiliation": "Stress Engineering Services (United States)"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "This paper provides an overview of the multi-image parallel features in Fortran 2023 and their implementation in the LLVM flang compiler and the Caffeine parallel runtime library. The features of interest support a Single-Program, Multiple-Data (SPMD) programming model based on executing multiple “images”, each of which is a program instance. The features also support a Partitioned Global Address Space (PGAS) in the form of “coarray” distributed data structures. The paper discusses the lowering of multi-image features to the Parallel Runtime Interface for Fortran (PRIF) and the implementation of PRIF in the Caffeine parallel runtime library. This paper also provides an early view into the design of a new multi-image dialect of the LLVM Multi-Level Intermediate Representation (MLIR). We describe validation and testing of the resulting software stack, and demonstrate that performance compares favorably to another open-source compiler and runtime library: GNU Compiler Collection (GCC) gfortran and OpenCoarrays, respectively.",
      "paperUrl": "https://doi.org/10.1145/3731599.3767480",
      "sourceUrl": "",
      "tags": [
        "Flang",
        "MLIR",
        "Performance",
        "Testing"
      ]
    },
    {
      "id": "openalex-w4416557611",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Improving Compiler Support for SIMD Offload Using Arm Streaming SVE",
      "authors": [
        {
          "name": "Mohamed Husain Noor Mohamed",
          "affiliation": "American Rock Mechanics Association"
        },
        {
          "name": "Adarsh Patil",
          "affiliation": "ARM (United Kingdom)"
        },
        {
          "name": "Latchesar Ionkov",
          "affiliation": "American Rock Mechanics Association"
        },
        {
          "name": "Eric Van Hensbergen",
          "affiliation": "American Rock Mechanics Association"
        }
      ],
      "year": "2025",
      "publication": "Lecture notes in computer science",
      "venue": "Lecture notes in computer science",
      "type": "research-paper",
      "abstract": "No abstract available in OpenAlex metadata.",
      "paperUrl": "https://doi.org/10.1007/978-3-032-07612-0_26",
      "sourceUrl": "",
      "tags": []
    },
    {
      "id": "openalex-w7129594072",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Implement Machine Learning to Schedule Instructions to Speed Up Execution: AI-Powered Microprocessor and Microcontroller Compiler Optimization",
      "authors": [
        {
          "name": "Ganesamoorthy Rajendran",
          "affiliation": "Chennai Mathematical Institute"
        },
        {
          "name": "Ghassan Samara",
          "affiliation": "Zarqa University"
        },
        {
          "name": "Johnsi Rani J",
          "affiliation": "Sri Venkateswara Veterinary University"
        },
        {
          "name": "Navyatha Ravi",
          "affiliation": "Birla Institute of Technology and Science - Hyderabad Campus"
        },
        {
          "name": "G. Karthikeyan",
          "affiliation": "Saveetha University"
        },
        {
          "name": "R. Ruth Shobitha",
          "affiliation": "TU Wien"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "Microprocessor and microcontroller are the key in today computing, in view of the important of executing instruction in a system, the instruction execution time is critical for the system performance and the time is known as instruction execution time. Classical optimization in compilers uses heuristic or static scheduling rules that can hardly adapt for increasing complexity due to instruction-level parallelism, heterogeneity, and real-time constraints. In this regard, machine learning (ML) is an attractive solution, which has the potential to provide intelligent, data-driven instruction scheduling and compiler optimizations that can take cognizance of diverse hardware and software state. This work presents a new approach that utilizes machine learning in the compiler pipeline for instruction scheduling of microprocessors and microcontrollers. The primary goal is to minimize runtime by predicting best instruction sequences according to runtime behaviors of programs, the architectural features of the processor and instruction execution history. By viewing instruction scheduling as a learning problem, we can use several popular ML models like decision trees, reinforcement learning agents, and graph neural networks to analyze instruction-dependencies, predict stalls, and generate schedules that minimize pipeline hazards and maximizes the usage of the available CPU resources. The approach is composed of offline learning with models trained using numerous compiled code traces, and online adaptation where reinforcement learning methods adapt scheduling policies at runtime or during recompilation. Training feature vectors includes instruction categories, dependency kat verses, memory access sen patterns, and branch probabilities. The resulting ML-aided compiler outputs optimized intermediates to be projected to the target assembly instructions, given by execution latency and power efficiency for embedded domains. Performance evaluation over benchmark suites, such as SPEC, MiBench, and custom embedded workloads indicates that the ML based scheduling approach can lead to 20-35 % better execution time reduction compared with baseline LLVM and GCC compilers. Furthermore, we also notice enhancements towards the instruction level parallelism and the reduction in register pressure, specially on loop/and control-flow-intensive code regions. The ML models show good transferability across microcontroller families (such as ARM Cortex-M, RISC-V cores) with minimal retraining. This work highlights the innovation potential of AI-assisted compiler design, especially within the real-time and low-power computation domain. By combining classic compiler theory with machine learning ideas, the research opens ways to intelligent automate processor designing and embedded systems developing. The work not only makes progress in instruction scheduling techniques, but also demonstrates a general concept for autonomous compiler optimization, of great importance for the era of AI-driven and heterogeneous computing.",
      "paperUrl": "https://doi.org/10.1109/punecon67554.2025.11379283",
      "sourceUrl": "",
      "tags": [
        "AI",
        "Embedded",
        "ML",
        "Optimizations",
        "Performance"
      ]
    },
    {
      "id": "openalex-w4416004180",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "ILAN: The Interference- and Locality-Aware NUMA Scheduler",
      "authors": [
        {
          "name": "Edvin Mellberg",
          "affiliation": "Chalmers University of Technology"
        },
        {
          "name": "Arvid Carlsson",
          "affiliation": "Chalmers University of Technology"
        },
        {
          "name": "Jing Chen",
          "affiliation": "Chalmers University of Technology"
        },
        {
          "name": "Miquel Pericàs",
          "affiliation": "University of Gothenburg"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "Modern HPC platforms increasingly adopt NUMA architectures, where OpenMP task-based programming model is a standard for enabling dynamic parallelism. However, the default OpenMP runtime is topology-agnostic, and the existing affinity policies are insufficient to ensure optimal performance on modern NUMA architectures. This lack of topology awareness results in suboptimal data locality and performance degradation. Additionally, the current OpenMP standard lacks mechanisms for detecting and mitigating the interference between concurrently executing tasks, further exacerbating the performance degradation. To enhance the performance of OpenMP task-based applications on NUMA architectures, we propose the ILAN scheduler: an interference- and locality-aware scheduler, employing moldability to dynamically minimize interference combined with hierarchical scheduling for improved data locality. We implement ILAN as an extension of LLVM OpenMP runtime. The results on a 64-core AMD Zen 4 platform show that ILAN achieves an average speedup of 13.2%, and a maximum speedup of 45.8% compared to the default scheduler.",
      "paperUrl": "https://doi.org/10.1145/3731599.3767701",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ]
    },
    {
      "id": "openalex-w4416770275",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "HalEngine: A Programming Language",
      "authors": [
        {
          "name": "K Nisarga",
          "affiliation": ""
        },
        {
          "name": "P Smitha",
          "affiliation": ""
        },
        {
          "name": "Reshma Hegde",
          "affiliation": ""
        },
        {
          "name": "N Harshitha",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "publication": "INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT",
      "venue": "INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT | Vol. 09 (Issue 11)",
      "type": "research-paper",
      "abstract": "Abstract -This paper represents HalEngine, a programming language which is designed to show how a compact design can still support practical programming tasks. The language is built using Python and LLVM and including features like variables, booleans, strings, basic math functions, conditionals, loops, functions with return-type inference, simple structures, and a minimal standard library. The compiler follows a straightforward architecture with a lexer, parser, abstract syntax tree, environment model, and LLVM IR generator. Despite its simplicity, HalEngine can compile and execute real programs while keeping the implementation easy to understand. Outlining the core design and the minimal runtime required to support the language, this language can be used to demonstrate how a functional compiler can be created with a small and focused feature set. Key Words: programming languages, syntax, compiler design, EBNF",
      "paperUrl": "https://ijsrem.com/download/halengine-a-programming-language/?wpdmdl=61574&refresh=69286922efd141764256034",
      "sourceUrl": "https://doi.org/10.55041/ijsrem54567",
      "tags": [
        "IR",
        "Programming Languages"
      ]
    },
    {
      "id": "openalex-w7117677369",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Graph Neural Network-Based Prediction of Soft Error Vulnerability and Criticality of Functions in Scientific Applications",
      "authors": [
        {
          "name": "Sanem Arslan Yılmaz",
          "affiliation": "Marmara University"
        }
      ],
      "year": "2025",
      "publication": "Gazi University Journal of Science Part A Engineering and Innovation",
      "venue": "Gazi University Journal of Science Part A Engineering and Innovation | Vol. 12 (Issue 4)",
      "type": "research-paper",
      "abstract": "Soft errors caused by transient hardware faults can lead to silent data corruptions (SDCs) in scientific applications, potentially impacting correctness and reliability. Traditional fault injection (FI) methods provide accurate vulnerability measurements but are prohibitively time-consuming and resource-intensive. In this work, we propose a function-level prediction framework for SDC vulnerability and criticality in CPU-based scientific applications using Graph Neural Networks (GNNs). Static code features are extracted from LLVM intermediate representation and used to construct function call graphs, enabling GCN, GAT, and GraphSAGE models to capture both intra-function characteristics and inter-function dependencies. The problem is formulated as both regression and classification, predicting continuous vulnerability and criticality scores as well as binary labels. The evaluation is conducted on 30 applications (90 functions) from the PolyBench benchmark suite using leave-one-application-out cross-validation, ensuring that the model is tested on unseen applications. Among the evaluated architectures, GraphSAGE achieves the highest performance (F1 = 0.80, MAE = 0.17), showing strong generalization across diverse workloads. Feature correlation and model-based importance analyses identify the most influential LLVM features, and results demonstrate that the proposed approach provides fine-grained, accurate predictions without the need for exhaustive FI campaigns, enabling more efficient and targeted fault-tolerance strategies.",
      "paperUrl": "https://dergipark.org.tr/en/download/article-file/5157720",
      "sourceUrl": "https://doi.org/10.54287/gujsa.1766028",
      "tags": [
        "Performance"
      ]
    },
    {
      "id": "openalex-w7127239984",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "GRP-0197 Evaluating the Ability of LLMs to Interpret, Optimize and Translate LLVM IR",
      "authors": [
        {
          "name": "Reuven Mueller",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "publication": "DigitalCommons - Kennesaw State University (Kennesaw State University)",
      "venue": "DigitalCommons - Kennesaw State University (Kennesaw State University)",
      "type": "research-paper",
      "abstract": "This study investigates whether modern state-of-the-art Large Language Models (LLMs) can interpret, optimize, and translate low-level intermediate representations (IR) used in compilers and binary translation software. We evaluate LLM performance on LLVM IR across three tasks: 1) interpreting the underlying algorithmic behavior, 2) identifying missed optimization opportunities, and generating improved IR variants, 3) Translating IR between AArch64 and x86-64 targets. To ensure correctness, all LLM generated IR is checked using a validation pipeline that verifies its syntax, structural correctness, compiles it into machine code, and executes it on randomized test data. Early results show that LLMs can perform non-trivial IR transformations on top of existing LLVM-O3 optimizations, highlighting their potential role in future compiler and binary translation workflows.",
      "paperUrl": "https://digitalcommons.kennesaw.edu/cday/Fall_2025/PhD_Research/5",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Optimizations",
        "Performance"
      ]
    },
    {
      "id": "openalex-w7113915030",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "From PyTorch to Calyx: An Open-Source Compiler Toolchain for ML Accelerators",
      "authors": [
        {
          "name": "Xie, Jiahan",
          "affiliation": ""
        },
        {
          "name": "Williams, Evan",
          "affiliation": ""
        },
        {
          "name": "Sampson, Adrian",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "publication": "ArXiv.org",
      "venue": "ArXiv.org",
      "type": "research-paper",
      "abstract": "We present an end-to-end open-source compiler toolchain that targets synthesizable SystemVerilog from ML models written in PyTorch. Our toolchain leverages the accelerator design language Allo, the hardware intermediate representation (IR) Calyx, and the CIRCT project under LLVM. We also implement a set of compiler passes for memory partitioning, enabling effective parallelism in memory-intensive ML workloads. Experimental results demonstrate that our compiler can effectively generate optimized FPGA-implementable hardware designs that perform reasonably well against closed-source industry-grade tools such as Vitis HLS.",
      "paperUrl": "https://arxiv.org/pdf/2512.06177",
      "sourceUrl": "http://arxiv.org/abs/2512.06177",
      "tags": [
        "CIRCT",
        "IR",
        "ML"
      ]
    },
    {
      "id": "openalex-w7127562772",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "FormalGym: Deep Reinforcement Learning Agent Based Formal Compiler Optimization Framework",
      "authors": [
        {
          "name": "Abhilash Majumder",
          "affiliation": "Nvidia (United Kingdom)"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "Selecting the correct sequence of compiler optimizations for LLVM - LLVM Phase Ordering- for an intermediate representation (IR) in terms of register interference and spillage, instruction counts, flops, correct codegen, optimal cfgs, and formally correct compiler construction is an area of active research. However ensuring that the sequence of compiler optimizations are formally correct require additional tuning of Deep Reinformcement Learning landscape.",
      "paperUrl": "https://doi.org/10.1145/3769002.3769975",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Optimizations"
      ]
    },
    {
      "id": "openalex-w7117898957",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Folded-tag: Enhancing memory safety with efficient hardware-supported memory tagging",
      "authors": [
        {
          "name": "Sumin Yang",
          "affiliation": "Korea University"
        },
        {
          "name": "Hongjoo Jin",
          "affiliation": "Korea University"
        },
        {
          "name": "Wonsuk Choi",
          "affiliation": "Korea University"
        },
        {
          "name": "Dong Hoon Lee",
          "affiliation": "Korea University"
        }
      ],
      "year": "2025",
      "publication": "Computers & Security",
      "venue": "Computers & Security | Vol. 163",
      "type": "research-paper",
      "abstract": "No abstract available in OpenAlex metadata.",
      "paperUrl": "https://doi.org/10.1016/j.cose.2025.104822",
      "sourceUrl": "",
      "tags": []
    },
    {
      "id": "openalex-w4417105794",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Enhancing concurrency bug detection in Rust programs through LLVM IR based graph visualization",
      "authors": [
        {
          "name": "Young Lee",
          "affiliation": ""
        },
        {
          "name": "Ernesto Aparicio Diaz",
          "affiliation": ""
        },
        {
          "name": "Jeong Yang",
          "affiliation": ""
        },
        {
          "name": "Bozhen Liu",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "publication": "High-Confidence Computing",
      "venue": "High-Confidence Computing",
      "type": "research-paper",
      "abstract": "No abstract available in OpenAlex metadata.",
      "paperUrl": "https://doi.org/10.1016/j.hcc.2025.100377",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Rust"
      ]
    },
    {
      "id": "openalex-w4416004221",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Energy-aware performance portability with OpenMP dynamic variants",
      "authors": [
        {
          "name": "Ayman Bourramouss el Maach",
          "affiliation": "Universitat Politècnica de Catalunya"
        },
        {
          "name": "Adrian Munera",
          "affiliation": "Universitat Politècnica de Catalunya"
        },
        {
          "name": "Sara Royuela",
          "affiliation": "Barcelona Supercomputing Center"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "Performance portability in HPC and embedded systems is often limited by power and thermal constraints. The OpenMP programming model offers a compile-time mechanism known as variants, allowing different function specializations. Previous research extended this concept to the runtime level, enabling dynamic variant selection. We build on these foundations with an energy‑aware runtime that augments variant selection with low‑overhead power and temperature instrumentation and a multi‑criteria policy balancing power caps, thermal headroom, and performance. Implemented in LLVM and publicly available, our mechanism profiles per‑variant energy and thermal behavior, selecting specializations at runtime based on user‑defined thresholds and live system state. Validation on HPC and embedded platforms shows the runtime enforces dynamic power caps with 98% compliance on a workstation (versus 67% unconstrained). On thermally constrained edge devices, proactive CPU/GPU migration beats hardware throttling, cutting execution time by 39% while maintaining stability. In a simulated battery‑limited mission, energy‑aware selection extends battery lifetime.",
      "paperUrl": "https://doi.org/10.1145/3731599.3767495",
      "sourceUrl": "",
      "tags": [
        "Embedded",
        "GPU",
        "Performance"
      ]
    },
    {
      "id": "openalex-w7117474336",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Efficiency of LLVM Compilers on High-Performance Architectures",
      "authors": [
        {
          "name": "Vedant Mohapatra",
          "affiliation": "Allen College"
        },
        {
          "name": "Sasmita Mohapatra",
          "affiliation": "The University of Texas at Dallas"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "The development of LLVM-based compilers introduces new possibilities for performance portability and modern standards in high-performance computing (HPC). This work evaluates their effectiveness specifically on AMD's GPU/CPU architectures, focusing on AMD Instinct accelerators. We assess compiler performance in terms of runtime efficiency, compilation overhead, and support for contemporary language features. The benchmarking suite includes both compute-bound and memorybound applications representative of real-world CPU/GPU-enabled HPC workloads. We compare several LLVM-driven compilers, such as Clang (from ROCm), AMD's AOCC and Intel's icx, as well as GNU's GCC and NVIDIA's NVCC, analyzing their ability to generate efficient code for AMD CPU/GPUs. This study offers practical insights for developers and compiler researchers aiming to optimize C applications on GNU, AMD, Intel CPU/GPU platforms.",
      "paperUrl": "https://doi.org/10.1109/ipccc66453.2025.11304659",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "GPU",
        "Performance"
      ]
    },
    {
      "id": "openalex-w7125905037",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "DebCovDiff: Differential Testing of Coverage Measurement Tools on Real-World Projects",
      "authors": [
        {
          "name": "W. Zhang",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Jinghao Jia",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Erkai Yu",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Darko Marinov",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Tianyin Xu",
          "affiliation": "University of Illinois Urbana-Champaign"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "Measuring code coverage is a critical practice in software testing. Incorrect or misleading coverage information reported by automatic tools can increase the software development cost and lead to negative consequences especially for safety-critical software. Ensuring the correctness of coverage measurement tools is therefore important. Prior studies have applied various techniques to find bugs in Gcov and LLVM-cov, the two most widely used coverage tools for C/C++. However, those studies had two limiting factors. First, they used only small, often synthetic, programs, potentially missing bugs in real-world scenarios. Second, they focused only on basic line coverage, neglecting advanced metrics that are both more complex to implement and commonly required for safety-critical software.This paper presents the first empirical study of coverage measurement tools for real-world projects. We implement DebCovDiff, a testing framework that takes Debian packages as the input programs and performs differential testing of Gcov and LLVM-cov, for line coverage and two advanced coverage metrics. We design robust differential oracles to (1) filter out discrepancies arising from subtle differences in the tool output presentation, (2) overcome the nondeterministic nature of certain packages, and (3) support advanced coverage metrics. From results on 47 Debian packages, we identify 34 new bugs, including 2 crashing bugs and 32 deeper bugs that produce wrong coverage reports.",
      "paperUrl": "https://doi.org/10.1109/ase63991.2025.00094",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Testing"
      ]
    },
    {
      "id": "openalex-w7116094757",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "DMGUARD: Safeguarding Kernels from Physical-Page Use-After-Free Vulnerabilities",
      "authors": [
        {
          "name": "Kim, Juhee",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Chung, Jaeyoung",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Jeong, Dae R.",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Lee, Byoungyoung",
          "affiliation": "Seoul National University"
        }
      ],
      "year": "2025",
      "publication": "Zenodo (CERN European Organization for Nuclear Research)",
      "venue": "Zenodo (CERN European Organization for Nuclear Research)",
      "type": "research-paper",
      "abstract": "This artifact contains the research artifacts for the paper \"DMGuard: Safeguarding Kernels from Physical-Page Use-After-Free Vulnerabilities\", to appear in USENIX Security 2026. DMGuard is a runtime mitigation system designed to comprehensively address physical-page use-after-free (UAF) vulnerabilities across diverse translation domains, including CPU, GPU, and IOMMU page tables. It utilizes a lightweight, lockless state machine to track physical page lifecycles and detect dangling mappings. Artifact Contents: Kernel Implementations: Source code patches for integrating DMGuard into: Linux kernel v6.6 (QEMU/ARM64) Android Linux kernel v6.1 (Pixel 8) Android Linux kernel v4.9 (Pixel 3 XL) Security Evaluation: Proof-of-Concept (PoC) codes for 10 real-world CVEs (covering io_uring, mm, usb, Mali GPU, Adreno GPU) and 5 synthetic vulnerabilities. Evaluation Scripts: Scripts for performance benchmarking (LMBench, Geekbench, etc.) and robustness testing (Android VTS kernel tests). System Requirements: Host: x86_64 machine running Ubuntu 22.04 LTS. Hardware: Google Pixel 3 XL (for Adreno security evaluation), Google Pixel 8 (for performance/robustness evaluation). Software Dependencies: LLVM/Clang 19+, Android NDK r26b+, Python 3.8+, and repo tool. Please refer to the README.md file included in the artifact for detailed instructions on environment setup, kernel compilation, and running the evaluations.",
      "paperUrl": "https://doi.org/10.5281/zenodo.17970502",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "GPU",
        "Performance",
        "Security",
        "Testing"
      ]
    },
    {
      "id": "openalex-w4417403177",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "CoroAMU: Unleashing Memory-Driven Coroutines through Latency-Aware Decoupled Operations",
      "authors": [
        {
          "name": "Zhuolun Jiang",
          "affiliation": "Institute of Computing Technology"
        },
        {
          "name": "Songyue Wang",
          "affiliation": "Institute of Computing Technology"
        },
        {
          "name": "X. Pei",
          "affiliation": "Institute of Computing Technology"
        },
        {
          "name": "Tianyue",
          "affiliation": "Institute of Computing Technology"
        },
        {
          "name": "Mingyu Chen",
          "affiliation": "Institute of Computing Technology"
        }
      ],
      "year": "2025",
      "publication": "arXiv (Cornell University)",
      "venue": "arXiv (Cornell University)",
      "type": "research-paper",
      "abstract": "Modern data-intensive applications face memory latency challenges exacerbated by disaggregated memory systems. Recent work shows that coroutines are promising in effectively interleaving tasks and hiding memory latency, but they struggle to balance latency-hiding efficiency with runtime overhead. We present CoroAMU, a hardware-software co-designed system for memory-centric coroutines. It introduces compiler procedures that optimize coroutine code generation, minimize context, and coalesce requests, paired with a simple interface. With hardware support of decoupled memory operations, we enhance the Asynchronous Memory Unit to further exploit dynamic coroutine schedulers by coroutine-specific memory operations and a novel memory-guided branch prediction mechanism. It is implemented with LLVM and open-source XiangShan RISC-V processor over the FPGA platform. Experiments demonstrate that the CoroAMU compiler achieves a 1.51x speedup over state-of-the-art coroutine methods on Intel server processors. When combined with optimized hardware of decoupled memory access, it delivers 3.39x and 4.87x average performance improvements over the baseline processor on FPGA-emulated disaggregated systems under 200ns and 800ns latency respectively.",
      "paperUrl": "https://arxiv.org/pdf/2511.14990",
      "sourceUrl": "https://doi.org/10.1109/pact65351.2025.00046",
      "tags": [
        "Performance"
      ]
    },
    {
      "id": "openalex-w4417432392",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Compiler-Assisted Optimization Using Neural Code Embeddings for Heterogeneous Architectures",
      "authors": [
        {
          "name": "Massimo Donelli",
          "affiliation": "Film Independent"
        }
      ],
      "year": "2025",
      "publication": "Journal of Multidisciplinary Knowledge",
      "venue": "Journal of Multidisciplinary Knowledge | Vol. 5 (Issue 2)",
      "type": "research-paper",
      "abstract": "The growing reliance on heterogeneous hardware (CPUs, GPUs, TPUs, NPUs) complicates compiler optimization because traditional rule-based heuristics cannot capture subtle performance interactions across architectures. This paper introduces NEO-Opt, a compiler-assisted optimization framework that integrates neural code embeddings and predictive performance models into the LLVM toolchain. Instead of relying exclusively on manually engineered heuristics, NEO-Opt learns optimization preferences from large corpora of real workloads and micro-benchmarks. The system represents code fragments using graph-based embeddings derived from control-flow and data-flow structures, which are fed into a multi-task predictor that estimates latency, memory pressure, and accelerator utilization. Based on these predictions, NEO-Opt selects optimization passes and scheduling policies dynamically. Evaluation across mixed computing platforms shows average performance gains of 12–22% for GPU-intensive workloads and up to 30% for compute-bound CPU kernels. Case studies illustrate how learned embeddings capture optimization opportunities missed by conventional compilers. We also analyze limitations, including embedding drift and poor generalization to rarely used instructions.",
      "paperUrl": "https://doi.org/10.36676/jmk.v5.i2.93",
      "sourceUrl": "",
      "tags": [
        "GPU",
        "Performance"
      ]
    },
    {
      "id": "openalex-w7106783238",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Can LLMs Recover Program Semantics? A Systematic Evaluation with Symbolic Execution",
      "authors": [
        {
          "name": "Feng, Rong",
          "affiliation": ""
        },
        {
          "name": "Saha, Suman",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "publication": "ArXiv.org",
      "venue": "ArXiv.org",
      "type": "research-paper",
      "abstract": "Obfuscation poses a persistent challenge for software engineering tasks such as program comprehension, maintenance, testing, and vulnerability detection. While compiler optimizations and third-party code often introduce transformations that obscure program intent, existing analysis tools and large language models (LLMs) struggle to recover the original semantics. In this work, we investigate whether LLMs, when fine-tuned with symbolic execution artifacts, can effectively deobfuscate programs and restore analyzability. We construct a benchmark by applying four widely studied transformations-control-flow flattening, opaque predicates, arithmetic encoding, and branch encoding-across diverse C programs from TUM Obfuscation Benchmarks, the LLVM test suite, and algorithmic repositories. We then compare three state-of-the-art LLMs under two training configurations: baseline fine-tuning on obfuscated/original code pairs, and enhanced fine-tuning with additional KLEE artifacts such as SMT constraints, path statistics, and test cases. Our evaluation examines syntactic correctness (compilation success), semantic fidelity (behavioral equivalence under symbolic execution), and code quality (readability and structure). Results show that GPT-4.1-mini achieves the strongest deobfuscation overall, and that incorporating KLEE artifacts consistently improves semantic preservation and compilation success across models. These findings highlight deobfuscation as a broader software engineering concern, demonstrating that combining LLMs with symbolic execution can strengthen automated testing, static analysis, and program comprehension in the presence of obfuscation.",
      "paperUrl": "https://arxiv.org/pdf/2511.19130",
      "sourceUrl": "http://arxiv.org/abs/2511.19130",
      "tags": [
        "Optimizations",
        "Static Analysis",
        "Testing"
      ]
    },
    {
      "id": "openalex-w4416548966",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "BLACKOUT: Data-Oblivious Computation with Blinded Capabilities",
      "authors": [
        {
          "name": "Hossam ElAtali",
          "affiliation": "University of Waterloo"
        },
        {
          "name": "Merve Gülmez",
          "affiliation": "Ericsson (Sweden)"
        },
        {
          "name": "Thomas Nyman",
          "affiliation": "Ericsson (Sweden)"
        },
        {
          "name": "N. Asokan",
          "affiliation": "University of Waterloo"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "Lack of memory-safety and exposure to side channels are two prominent, persistent challenges for the secure implementation of software. Memory-safe programming languages promise to significantly reduce the prevalence of memory-safety bugs, but make it more difficult to implement side-channel-resistant code. We aim to address both memory-safety and side-channel resistance by augmenting memory-safe hardware with the ability for data-oblivious programming. We describe an extension to the CHERI capability architecture to provide blinded capabilities that allow data-oblivious computation to be carried out by userspace tasks. We also present BLACKOUT, our realization of blinded capabilities on a FPGA softcore based on the speculative out-of-order CHERI-Toooba processor and extend the CHERI-enabled Clang/LLVM compiler and the CheriBSD operating system with support for blinded capabilities. BLACKOUT makes writing side-channel-resistant code easier by making non-data-oblivious operations via blinded capabilities explicitly fault. Through rigorous evaluation we show that BLACKOUT ensures memory operated on through blinded capabilities is securely allocated, used, and reclaimed and demonstrate that, in benchmarks comparable to those used by previous work, BLACKOUT imposes only a small performance degradation (1.5% geometric mean) compared to the baseline CHERI-Toooba processor.",
      "paperUrl": "https://doi.org/10.1145/3719027.3765169",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Performance",
        "Programming Languages"
      ]
    },
    {
      "id": "openalex-w4416557590",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "Automatically Parallelizing Batch Inference on Deep Neural Networks Using Fiats and Fortran 2023 “Do Concurrent”",
      "authors": [
        {
          "name": "Damian Rouson",
          "affiliation": "Lawrence Berkeley National Laboratory"
        },
        {
          "name": "Zhe Bai",
          "affiliation": "Lawrence Berkeley National Laboratory"
        },
        {
          "name": "Dan Bonachea",
          "affiliation": "Lawrence Berkeley National Laboratory"
        },
        {
          "name": "Kareem Ergawy",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "E. D. Gutmann",
          "affiliation": "NSF National Center for Atmospheric Research"
        },
        {
          "name": "Michael Klemm",
          "affiliation": "Advanced Micro Devices (Canada)"
        },
        {
          "name": "Katherine Rasmussen",
          "affiliation": "Lawrence Berkeley National Laboratory"
        },
        {
          "name": "Brad Richardson",
          "affiliation": "Lawrence Berkeley National Laboratory"
        },
        {
          "name": "Sameer Shende",
          "affiliation": "University of Oregon"
        },
        {
          "name": "David J. Torres",
          "affiliation": "Northern New Mexico College"
        },
        {
          "name": "Yunhao Zhang",
          "affiliation": "Lawrence Berkeley National Laboratory"
        }
      ],
      "year": "2025",
      "publication": "Lecture notes in computer science",
      "venue": "Lecture notes in computer science",
      "type": "research-paper",
      "abstract": "No abstract available in OpenAlex metadata.",
      "paperUrl": "https://doi.org/10.1007/978-3-032-07612-0_11",
      "sourceUrl": "",
      "tags": []
    },
    {
      "id": "openalex-w7115607765",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "An LLVM-Based Optimization Pipeline for SPDZ",
      "authors": [
        {
          "name": "Dai, Tianye",
          "affiliation": ""
        },
        {
          "name": "Mendes, Hammurabi",
          "affiliation": ""
        },
        {
          "name": "Lim, Heuichan",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "publication": "ArXiv.org",
      "venue": "ArXiv.org",
      "type": "research-paper",
      "abstract": "Actively secure arithmetic MPC is now practical for real applications, but performance and usability are still limited by framework-specific compilation stacks, the need for programmers to explicitly express parallelism, and high communication overhead. We design and implement a proof-of-concept LLVM-based optimization pipeline for the SPDZ protocol that addresses these bottlenecks. Our front end accepts a subset of C with lightweight privacy annotations and lowers it to LLVM IR, allowing us to reuse mature analyses and transformations to automatically batch independent arithmetic operations. Our back end performs data-flow and control-flow analysis on the optimized IR to drive a non-blocking runtime scheduler that overlaps independent operations and aggressively overlaps communication with computation; when enabled, it can map batched operations to GPU kernels. This design preserves a low learning curve by using a mainstream language and hiding optimization and hardware-specific mechanics from programmers. We evaluate the system on controlled microbenchmarks against MP-SPDZ, focusing on online phase performance. Our CPU back end achieves up to 5.56 times speedup under intermediate and heavy algebraic workloads, shows strong scaling with thread count, and our GPU back end scales better as the input size increases. Overall, these results indicate that leveraging LLVM with protocol-aware scheduling is an effective architectural direction for extracting parallelism without sacrificing usability.",
      "paperUrl": "https://arxiv.org/pdf/2512.11112",
      "sourceUrl": "http://arxiv.org/abs/2512.11112",
      "tags": [
        "GPU",
        "IR",
        "Performance"
      ]
    },
    {
      "id": "openalex-w7123335560",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "An Analysis Methodology for Implicit Interfaces in Safety-Critical Embedded Software: Theory and Application",
      "authors": [
        {
          "name": "Jie Ding",
          "affiliation": "Beihang University"
        },
        {
          "name": "Xiaohong Bao",
          "affiliation": "Beihang University"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "As the complexity and scale of safety-critical embedded systems continue to grow, traditional interface specifications and verification methods are increasingly insufficient to cope with safety risks posed by implicit interfaces–interaction paths that are not explicitly declared yet exist in practice. This paper (i) introduces an operational definition of implicit interfaces and a multi-dimensional taxonomy, (ii) analyzes implicit interfaces from multiple perspectives and proposes safeguards spanning the entire software life cycle, and (iii) develops a static safety-analysis approach that combines LLVM intermediate representation (IR) with symbol-table reasoning for two representative mechanisms-shared-variable coupling and symbol binding-together with a prototype tool. Through an empirical study on three representative open-source embedded projects, the method efficiently and accurately uncovers implicit-interface risks that elude existing tools. Results show that our approach provides a scalable theoretical framework and practical tooling for assuring the safety and reliability of safety-critical embedded software in the presence of implicit interfaces.",
      "paperUrl": "https://doi.org/10.1109/dsa66321.2025.00027",
      "sourceUrl": "",
      "tags": [
        "Embedded",
        "IR"
      ]
    },
    {
      "id": "openalex-w4416182184",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "A vulnerability detection method for obfuscated binary programs based on anti-obfuscation intrinsic features",
      "authors": [
        {
          "name": "Qi Meng",
          "affiliation": "PLA Information Engineering University"
        },
        {
          "name": "Xuemeng Wang",
          "affiliation": "PLA Information Engineering University"
        },
        {
          "name": "Minghao Chen",
          "affiliation": "PLA Information Engineering University"
        },
        {
          "name": "Jinlong Fei",
          "affiliation": "PLA Information Engineering University"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "With the continuous expansion and increasing complexity of software development, software obfuscation techniques have been widely adopted to protect intellectual property and enhance security. However, obfuscation also introduces significant challenges for vulnerability detection, causing traditional binary analysis methods to exhibit substantial performance degradation when processing obfuscated binary programs. Additionally, the presence of various compiler optimization techniques further exacerbates the difficulty of vulnerability detection in obfuscated programs. To address these challenges, we propose a vulnerability detection method for obfuscated binary programs based on intrinsic antiobfuscation features. The proposed method first extracts and filters intrinsic anti-obfuscation features, preserving those that remain highly robust and distinctive even after obfuscation. These extracted features are then standardized using a feature normalizer before being fed into a LongFormer model for feature encoding representation. Subsequently, cosine similarity calculations are performed on the encoded intrinsic anti-obfuscation feature vectors. Our proposed intrinsic anti-obfuscation features are easily extractable, robust, and independent of compilation toolchains and other binary transformation processes. Moreover, this approach demonstrates strong resilience against compiler optimizations and widely used O-LLVM code obfuscation techniques, making it a practical and effective solution for obfuscated binary vulnerability detection.",
      "paperUrl": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13985/139852N/A-vulnerability-detection-method-for-obfuscated-binary-programs-based-on/10.1117/12.3081567.pdf",
      "sourceUrl": "https://doi.org/10.1117/12.3081567",
      "tags": [
        "Optimizations",
        "Performance",
        "Security"
      ]
    },
    {
      "id": "openalex-w4415924552",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "A cross-platform execution engine for the quantum intermediate representation",
      "authors": [
        {
          "name": "Elaine Wong",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Vicente Leyton‐Ortega",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Daniel Claudino",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "S. R. Johnson",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Austin J. Adams",
          "affiliation": "Georgia Institute of Technology"
        },
        {
          "name": "Sharmin Afrose",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Meenambika Gowrishankar",
          "affiliation": "Knoxville College"
        },
        {
          "name": "Anthony M. Cabrera",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Travis S. Humble",
          "affiliation": "Oak Ridge National Laboratory"
        }
      ],
      "year": "2025",
      "publication": "The Journal of Supercomputing",
      "venue": "The Journal of Supercomputing | Vol. 81 (Issue 16)",
      "type": "research-paper",
      "abstract": "Hybrid languages like the quantum intermediate representation (QIR) are essential for programming systems that mix quantum and conventional computing models, while execution of these programs is often deferred to a system-specific implementation. Here, we develop the QIR Execution Engine (QIR-EE) for parsing, interpreting, and executing QIR across multiple hardware platforms. QIR-EE uses LLVM to execute hybrid instructions specifying quantum programs and, by design, presents extension points that support customized runtime and hardware environments. We demonstrate an implementation that uses the XACC quantum hardware-accelerator library to dispatch prototypical quantum programs on different commercial quantum platforms and numerical simulators, and we validate execution of QIR-EE on IonQ, Quantinuum, and IBM hardware. Our results highlight the efficiency of hybrid executable architectures for handling mixed instructions, managing mixed data, and integrating with quantum computing frameworks to realize cross-platform execution.",
      "paperUrl": "https://link.springer.com/content/pdf/10.1007/s11227-025-07969-2.pdf",
      "sourceUrl": "https://doi.org/10.1007/s11227-025-07969-2",
      "tags": [
        "Quantum Computing"
      ]
    },
    {
      "id": "openalex-w7118051364",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "A Magnified View into Heterogeneous-ISA Thread Migration Performance without State Transformation",
      "authors": [
        {
          "name": "Nikolaos Mavrogeorgis",
          "affiliation": ""
        },
        {
          "name": "Christos Vasiladiotis",
          "affiliation": ""
        },
        {
          "name": "Pei Mu",
          "affiliation": ""
        },
        {
          "name": "Amir Khordadi",
          "affiliation": ""
        },
        {
          "name": "Björn Franke",
          "affiliation": ""
        },
        {
          "name": "Antonio Barbalace",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "publication": "ArXiv.org",
      "venue": "ArXiv.org",
      "type": "research-paper",
      "abstract": "Heterogeneous-ISA processor designs have attracted considerable research interest. However, unlike their homogeneous-ISA counterparts, explicit software support for bridging ISA heterogeneity is required. The lack of a compilation toolchain ready to support heterogeneous-ISA targets has been a major factor hindering research in this exciting emerging area. For any such compiler, \"getting right\" the mechanics involved in state transformation upon migration and doing this efficiently is of critical importance. In particular, any runtime conversion of the current program stack from one architecture to another would be prohibitively expensive. In this paper, we design and develop Unifico, a new multi-ISA compiler that generates binaries that maintain the same stack layout during their execution on either architecture. Unifico avoids the need for runtime stack transformation, thus eliminating overheads associated with ISA migration. Additional responsibilities of the Unifico compiler backend include maintenance of a uniform ABI and virtual address space across ISAs. Unifico is implemented using the LLVM compiler infrastructure, and we are currently targeting the x86-64 and ARMv8 ISAs. We have evaluated Unifico across a range of compute-intensive NAS benchmarks and show its minimal impact on overall execution time, where less than 6% (10%) overhead is introduced on average for high-end (low-end) processors. We also analyze the performance impact of Unifico's key design features and demonstrate that they can be further optimized to mitigate this impact. When compared against the state-of-the-art Popcorn compiler, Unifico reduces binary size overhead from ~200% to ~10%, whilst eliminating the stack transformation overhead during ISA migration.",
      "paperUrl": "https://arxiv.org/pdf/2512.24530",
      "sourceUrl": "http://arxiv.org/abs/2512.24530",
      "tags": [
        "Backend",
        "Infrastructure",
        "Performance"
      ]
    },
    {
      "id": "openalex-w7129696276",
      "source": "openalex-llvm-search-page1",
      "sourceName": "OpenAlex Query (title/abstract: llvm, page 1)",
      "title": "A Hierarchical Address Decomposition Method for Multidimensional Array Access Optimization",
      "authors": [
        {
          "name": "Jingming Gou",
          "affiliation": "China West Normal University"
        },
        {
          "name": "Xianbo He",
          "affiliation": "China West Normal University"
        }
      ],
      "year": "2025",
      "publication": "",
      "venue": "",
      "type": "research-paper",
      "abstract": "Multidimensional array access is a primary performance bottleneck in High-Performance Computing (HPC). Modern compilers typically flatten multi-dimensional index computations into a single offset expression, thereby obscuring the hierarchical invariance inherent in the address calculation that corresponds to the loop nesting structure. This flattening prevents standard optimizations like Loop-Invariant Code Motion (LICM) from being effective. This paper proposes a Hierarchical Address Decomposition (HAD) method, which reconstructs address computations in LLVM into a chained intermediate representation that is isomorphic to the loop nest. This transformation explicitly exposes pointer-level invariants, enabling them to be automatically hoisted by the standard LICM pass. Furthermore, this paper introduces a model-guided promotion strategy that leverages a cost model to balance the benefits of optimization against the overhead of the transformation. Experimental results on SPEC CPU bench-marks (481.wrf and 621.wrf_s) demonstrate that HAD achieves up to a 1.08x speedup, validating its effectiveness and portability for multidimensional array access optimization.",
      "paperUrl": "https://doi.org/10.1109/icaice68195.2025.11382376",
      "sourceUrl": "",
      "tags": [
        "Optimizations",
        "Performance"
      ]
    }
  ]
}
